\chapter{Анализ производительности узлов мультиархитектурной ВС} \label{chapt3}
Описаны методы, позволяющие определять скорость счета на ускорителях вычислений и скорость перемещения данных между ускорителем вычислений и хост-машиной, а также давать прогнозы о скорости счета нереализованных еще алгоритмов на тестируемой ВС.

Кроме того, предложена методика оценки качества узлов мультиархитектурной ВС на основе графических (или других) ускорителей, при этом качество понимается как сбалнсированность средней оценочной скорости счета на ускорителе и скорости пермещения данных между ускорителем и хостом
\cite{MohographyTarkov,VestnikNNSU,vakbib2,VychMethProgExa,SuperFrI,astroCoDesign,integrApproach}.

 


\section{Анализ производительности узлов с графическими ускорителями} \label{sect3_1}

\begin{table}[ht]
	\begin{center}
		\caption{Основные параметры GPU, использованных в тестах}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			Название          &GeForce  & M2090  & K40m    & K80    & P100  \\ \hline
			                  & GTX 850M&        &         &        &
			Объем             &         &        &         &        &       \\
			глобальной памяти & 4 GB    & 5.3GB  & 11.5GB  & 16 GB  & 16 GB \\ \hline
			Количество        &         &        &         &        &       \\
			мультипроцессоров & 5       &  16    & 15      & 26     &  56   \\ \hline
			Количество        &         &        &         &        &       \\
			ядер CUDA         & 640     & 512    & 2880    &  4992  & 3584  \\ \hline
			Тактовая частота  &         &        &         &        &        \\
			GPU               & 902 MHz &1301 MHz& 745 MHz & 560 MHz&  1328 MHz \\ \hline
			Ширина            &         &        &         &        &         \\
			шины памяти       & 128-bit & 384-bit& 384-bit & 384-bit&  4096-bit\\ \hline
		\end{tabular}
		\label{GPUs-params}
	\end{center}
\end{table}





В таблице \ref {PerfGPUs} приведены времена для различных частей алгоритма на трех разных GPU, параметры которых приведены в таблице \ref{GPUs-params}. Все упомянутые в таблице \ref {PerfGPUs}, кроме GeForce GTX 850M, являются представителями продуктовой линейки Tesla, и полное название выглядело бы например, так: Tesla P100. Тем не менее название Tesla не приводится в таблице для экономии места.

  Здесь важны не столько сами числа, но и возможность проанализировать различия и предложить новую стратегию оптимизации. Во-первых, видно, что вычисление полей работает быстрее с K40. Это означает, что регулярный доступ к памяти должен быть обеспечен везде, чтобы достигнуть высокой производительности. Во-вторых, сдвиг частиц выполняется почти в 10 раз медленнее на  GeForce, у которого больше ядер, чем у M2090. Это означает, что программа зависит, главным образом, от ширины шины памяти
и числа мультипроцессоров. В-третьих, времена присваивания токов  близки для M2090 и K40. Это означает, что не все ядра используются (на K40 их в 5 раз больше!).

\begin{table}[ht]
	\begin{center}
		\caption{Время выполнения основных процедур на различных GPU (оптимизированный вариант), в миллисекундах}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Марка GPU        &  GeForce GTX 850M & M2090 & K40 \\ \hline
			Сдвиг частиц       &  2000           &  211.082    & 285.839 \\ \hline
			Вычисление поля    &  1.004          &  1.195      & 0.353   \\ \hline
			Переупорядочивание  &                 &             &         \\
			частиц            &  2226           &  220.623    & 191.298  \\ \hline
			Присваивание токов &  316            &  22.67      & 44 \\ \hline
			Присвавание полей   &  169            &  11.983     & 8.066 \\ \hline
		\end{tabular}
		\label{PerfGPUs}
	\end{center}
\end{table}



В таблице \ref{tabP100} приведено время работы основных частей алгоритма на K40, K80 (использован 1 GPU) по сравнению с P100.

Обращает на себя внимание тот факт, что движение модельных частиц на P100 вычисляется даже немного медленнее, чем на K80, впрочем, разницу (10 мкс) можно отнести на счет погрешности измерений. В то же время значительно ускорен расчет магнитного поля и, что наиболее важно, накладные расходы, связанные с переупорядочиванием модельных частиц, стали намного меньше. Это означает, что  переход на P100 может значительно ускорить расчеты с использованием метода частиц.

\begin{table}[ht]
	\caption{ Основные части вычислительного алгоритма на различных GPU, в миллисекундах.}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Название GPU & K40 & K80 & P100\\\hline
			Вычисление электрического поля      & 6    & 4.6     & 2.5    \\\hline
			Движение частиц                     &10500 & 294.24  & 306.9  \\\hline
			Вычисление магнитного поля (1 этап) & 190  & 67.3    & 14.7   \\\hline
			Вычисление магнитного поля (2 этап) & 112  & 40.277  & 10.4   \\ \hline
			Переупорядочивание частиц (1 этап)  & 16   & 33.72   & 10.24  \\ \hline
			Переупорядочивание частиц (2 этап)  & 1.13 & 444.7   & 81.6       \\ \hline
		\end{tabular}
	\end{center}
	\label{tabP100}
\end{table}


Основной вопрос данного раздела, как и всей работы - что можно узнать о данной ВС путем запуска программы, реализующей метод частиц в ячейках? В отличие от 	большинства других разделов информация о характеристиках оборудования в данном случае доступна через стандартный интерфейс, соответственно фактически измеренную скорость счета и скорость пересылки данных между хостом и GPU можно сравнивать с номинальными показателями.

Аналогично разделу \ref{calc_PE} определяется производительность GPU во флопсах как для этапа расчета частиц, так и для этапа расчета электромагнитного поля.

Важнейшей интегральной характеристикой ВС, оснащенной графическими ускорителями, является возможность их полноценно использовать. Эта возможность
может быть измерена с помощью сопоставления вычисленной скорости счета и скорости пересылок данных между хостом и GPU с использованием описанного 
в разделе \ref{complex_evaluation} переводного множителя $k_{f2b}$, формула \ref{kf2b}. Этот множитель отражает принципиальную возможность переслать необходимые данные с GPU на хост и далее по коммуникационной сети ВС на соседние узлы раньше, чем они понадобятся для счета на соседнем узле, и таким образом счет может продолжать без задержек, вызванных комммуникациями.

Вместе с тем вопрос, который наиболее часто задают специалисты по математическому моделированию применительно к мультиархитектурной ВС, оснащенной графическими ускорителями - это возможность \textit{эффективной} реализации конкретного вычислительного алгоритма на даннной мультиархитектурной ВС.
Для ответа на данный вопрос предлагается интерполяционная формула:
\begin{equation}
v_{pre} = v_{PIC} k + (1-k) v_{B,E}
\end{equation} 
здесь $ v_{pre}$ - оценка скорости вычислений на GPU для рассматриваемого алгоритма, $v_{PIC}$ - скорость вычислений с на этапе сдвига модельных частиц, $v_{B,E}$ - на этапе расчета электромагнитного поля, а $k$ - интерполяционный множитель, получаемый из следующих соображений.

Как уже говорилось выше, большинство численных методов используемых в математическом моделировании находятся в промежуточном положении по отношению к используемым в методе частиц в ячейках алгоритму вычисления поля и алгоритму расчета движения частиц по следующим показателям:
\begin{itemize}
	\item вычислительной интенсивности (равномерное распределение вычислительно сложных фрагментов по тексту или отдельные высоконагруженные участки);
	\item характеру доступа к оперативнной памяти (регулярный или нерегулярный);
	\item объему используемых данных (большой или маленький).
\end{itemize}
Ориентировочное распределение вычислительных алгоритмов по рассмотренным показателям и соответствующие значения коэффициента $k$ показаны в таблице \ref{tab-interp-koef}


\begin{table}[ht]
	\begin{center}
		\caption{Определение интерполяционного коэффициента для некоторых типов вычислительных алгоритмов}
		\begin{tabular}{|c|c|c|c|c|}
			%	        &   &  &  & k \\ \hline
			\hline
			Вычислительный & Интенсивность &  Доступ к     & Объем  & $k$  \\ 
		    	алгоритм   &               &   оперативной   & данных&  \\
		                   &               &   памяти        &       &  \\ \hline
			
			
			Расчет движения  &  низкая & нерегулярный & большой &
			1.0 \\ 
			модельных частиц                            &         &             &          & \\\hline
			Метод Монте-Карло                &  низкая & нерегулярный & средний & 0.9 \\ \hline
			Метод SPH    &  низкая & нерегулярный & небольшой & 0.6 \\ \hline
			Метод              &  высокая & нерегулярный & большой & 0.5  \\
			конечных элементов &          &              &         & \\ \hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.2 \\ 		
			схемы (явные)      &           &            &         &     \\\hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.1 \\ 		
			схемы (явные)-2    &           &            &         &     \\\hline
			
			Вычисление         &  высокая  & регулярный & большой & 0.0 \\ 		
			электромагнитного поля      &           &            &         &     \\\hline
			
			
		\end{tabular} 
		\label{tab-interp-koef}              
	\end{center}
\end{table}

Здесь необходимо дать конкретизацию слов <<большой>>, <<средний>>, <<высокая>>, <<низкая>>. На основе опыта работы с различными классами приложений, можно сказать, что высокая интенсивность доступа к памяти - это 10 и более ГБ/сек., большой объем оперативной памяти - десятки терабайт и выше, средний - от 10 ГБ.  

\section{Реализация программы теста на различных типах мультиархитектурных ВВС}

Для того, чтобы обеспечить возможность тестирования ВВС всех доступных архитектур (в частности, тех, что созданы на базе GPU, и тех что на базе CPU, и тех что основаны на ускорителях вычислений) на базе одной и той же программы, необходимо обеспечить возможность переноса программы между указанными типами архитектур.
Технология переноса программ численного моделирования с GPU на Intel Xeon Phi
будет показана на примере программы для моделирования динамики плазмы методом частиц в ячейках.

Вначале необходимо ответить на вопрос, для чего нужна такая методика?

Во-первых, необходимо иметь возможность использовать наиболее мощные гибридные суперЭВМ, а такие сейчас строятся (в том числе) на базе Intel Xeon Phi. Кроме того, в
докладе А.О.Лациса “Что же делать с этим многообразием суперкомпьютерных миров?” на конференции “Научный сервис в сети Интернет-2014” \cite{Lacis2014} была предложена методика создания единого переносимого программного обеспечения для решения вычислительных задач, которое могло бы использоваться на многих суперкомпьтерных архитектурах. Эта методика основана на использовании библиотеки BLAS, которая так или иначе существует на всех машинах.

Большое количество различных суперкомпьютерных архитектур приводит к необходимости разрабатывать отдельный вариант программы под каждую из них.
В то же время наиболее распространенные в настоящее время суперкомпьютерные архитектуры строятся на основе одних и тех же принципов, т.е. кластеры с использованием ускорителей вычислений или просто кластеры.
Это означает, что задача создания инструмента для облегченного (упрощенного), хотя и не автоматического перехода между двумя разными  суперкомпьютерными архитектурами
представляется осуществимой.

Вопросы портирования программ на Intel Xeon Phi, в частности, рассматриваются в \cite{Rosales2Phi}. Кроме того, в работе \cite{Nakashima2015} изучается проблема достижения максимальной заявленной производительности в 1 Teraflops с помощью ускорителя Intel Xeon Phi. Сравнение производительности ускорителей Intel Xeon Phi и графических ускорителей на различных задачах проведено в \cite{Lyakh201584,Liu2015230,Bernaschi20142495}.

Новизна методики переноса, созданной в диссертации заключается в разработке полуавтоматического средства переноса программ,
которое с одной стороны, было бы эффективным,
с другой стороны, обеспечивало бы полный контроль над процессом переноса для прикладного программиста.

\subsection{Постановка задачи}

Необходимо решить вопрос о переносе между наиболее распространенными (как в России, так и в мире) типами суперкомпьютерных архитектур:
\begin{enumerate}
	\item Кластера на основе Nvidia Kepler; 
	\item Кластера на основе Intel Xeon Phi;
	\item Кластера на основе Intel Xeon;
\end{enumerate}
Рассматривается перенос программы с GPU на Intel Xeon Phi (не наоборот!!!) и не рассматривается на данный момент вопрос оптимизации под ту или иную архитектуру.

Основные проблемы переноса с архитектуры CUDA\cite{CUDAweb,Boreskov,Sanders} на архитектуру MIC\cite{MorganPhi,FangPhi2014}.
\begin{enumerate}
	\item Компиляция ядер CUDA 
	и в особенности вызовов ядер CUDA без компилятора Nvidia;
	\item Пропуск операций копирования между различными видами памяти в CUDA;
	\item Определение типов данных и ключевых слов, входящих в расширение языка C, используемое в CUDA.
\end{enumerate}


\begin{itemize}
	\item Архитектурно-зависимые участки кода: 
	\begin{itemize}
		\item Сводятся к минимуму;
		\item Оформляются в виде процедур; 
		\item Выносятся во внешнюю подключаемую библиотеку.
	\end{itemize}
	\item Таким образом в тексте программы присутствует некий обобщенный вызов процедуры, который приобретает конкретную форму при компиляции в зависимости:
	\begin{itemize}
		\item От компилятора;
		\item От архитектуры.
	\end{itemize}
\end{itemize}

Сведение к минимуму архитектурно-зависимых участков кода выполняется следующим образом. 
В коде имеется 15-20 вызовов небольших вычислительных процедур, 
выполняющих обработку:
\begin{itemize}
	\item узлов сетки;
	\item модельных частиц;
	\item границ расчетной области.
\end{itemize}
Эти процедуры оформлены в виде ядер CUDA. Таким образом, эти процедуры не могут быть скомпилированы с помощью компилятора Intel и пр.
Основной принцип предлагаемой методики переноса программ: \textbf{сделать такой участок кода по крайней мере единственным}.

\subsection{Универсальная процедура запуска}

В качестве реализации сформулированного выше принципа (вынести все непереносимые элементы кода в одну процедуру) предлагается универсальная процедура запуска, которая показана на листинге \ref{universal-launcher}. На вход этой процедуре в качестве параметра подаются процедуры, которые запускаются из-под ядер CUDA. Далее в том случае, если этот код компилируется компилятором CUDA C/C++ и исполняется на машине с GPU, то процедурный параметр передается универсальному ядру (GPU\_Universal\_Kernel на листинге \ref{universal-launcher}) и запускается на GPU внутри ядра. Если же этот код компилируется компилятором Intel (например) и исполняется
на ускорителе Intel Xeon Phi (в режиме native) или просто на многоядерном процессоре под OpenMP, то переданная в качестве параметра процедура будет просто вызываться в цикле, таким образом обеспечивая тот же результат, что и при запуске на GPU. Цикл в данном случае имеет шестикратную степень вложенности (соответствующую шести размерностям сетки потоковых блоков, используемой в CUDA - три размерности сетки и три размерности потокового блока).
\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
	\caption{Универсальная процедура запуска}
	\label{universal-launcher}	
	\begin{lstlisting}[language={[ISO]C++}]
	
	int Kernel_Launcher(
	Cell<Particle>  **cells,KernelParams *params,
	unsigned int grid_size_x,unsigned int grid_size_y,unsigned int grid_size_z,
	unsigned int block_size_x,unsigned int block_size_y,unsigned int block_size_z,
	int shmem_size,
	SingleNodeFunctionType h_snf,char *name)
	{
	struct timeval tv1,tv2;
	#ifdef __CUDACC__
	dim3 blocks(grid_size_x,grid_size_y,grid_size_z),threads(block_size_x,block_size_y,block_size_z);
	
	gettimeofday(&tv1,NULL);
	GPU_Universal_Kernel<<<blocks,threads,shmem_size>>>(cells,params,h_snf);
	DeviceSynchronize();
	gettimeofday(&tv2,NULL);
	#else
	char hostname[1000];
	gethostname(hostname,1000);
	
	#ifdef OMP_OUTPUT
	printf("function %s executed on %s \n",name,hostname);
	#endif
	
	gettimeofday(&tv1,NULL);
	
	omp_set_num_threads(OMP_NUM_THREADS);
	
	#pragma omp parallel for
	for(int i = 0;i < grid_size_x;i++)
	{
	//      ....              
	h_snf(cells,params,i,j,k,i1,j1,k1);
	// ....
	}
	}
	\end{lstlisting}
\end{ListingEnv}

\subsection{Унифицированная сигнатура расчетных процедур}
Все расчетные процедуры, которые при исполнении программы на GPU запускаются как ядра CUDA, должны быть оформлены в виде процедур с единой сигнатурой (одинаковый тип возвращаемого значения и одинаковый набор параметров), показанной на листинге \ref{listing-signature}.

Далее необходимо отработать расширения языка C, используемые в CUDA С/С++, как-то специальные типы данных
(int3, double3, dim3, и пр.). Их можно либо доопределить, либо при возможности скопировать файл cuda.h, если на данной машине установлена библиотека CUDA. Специальные ключевые слова: \_\_global\_\_ ,         \_\_device\_\_, и др. можно 
замаскировать с помощью директив условной компиляции.

Остаются функции CUDA API: копирование из одного типа памяти в другой, обработка ошибок и пр. Эти функции должны вызываться не напрямую, как CUDA API, а через функции-обертки, вынесенные в отдельный заголовочный файл.

\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
\caption{Тип универсальной счетной процедуры}
\label{listing-signature}
	\begin{lstlisting}[language={[ISO]C++}]
	typedef void (*SingleNodeFunctionType)(GPUCell<Particle>  **cells,KernelParams *params,
	unsigned int bk_nx,unsigned int bk_ny,unsigned int bk_nz,
	unsigned int nx,unsigned int ny,unsigned int nz
	);
	
	\end{lstlisting}
\end{ListingEnv}

Этот процесс будет показан на примере процедуры расчета электрического поля из класса GPU-пространство моделирования, листинг \ref{listing-GPU-plasma-class}. На листинге \ref{Original-function} показано первоначально имеющееся ядро CUDA, предназначенное для вычисления электрического поля в одном узле сетки. При этом реальные вычисления проводятся процедурой emeElement. Само ядро только лишь определяет узел сетки с помощью внутренних индексов CUDA, передает параметры (шаги сетки, временной шаг, магнитное поле, соответсвующую компоненту тока, как описано в разделе \ref{beam-plasma-methods}) и вызывает процедуру emeElement.


\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
\caption{Первоначально имеющееся ядро CUDA, предназначенное для вычисления электрического поля.}
\label{Original-function}	
	\begin{lstlisting}[language={[ISO]C++}]
	template <template <class Particle> class Cell >
	__global__ void GPU_eme(
	
	Cell<Particle>  **cells,
	int i_s,int l_s,int k_s,
	double *E,double *H1, double *H2,
	double *J,double c1,double c2, double tau,
	int dx1,int dy1,int dz1,int dx2,int dy2,int dz2
	)
	{
	unsigned int nx = blockIdx.x*blockDim.x + threadIdx.x;
	unsigned int ny = blockIdx.y*blockDim.y + threadIdx.y;
	unsigned int nz = blockIdx.z*blockDim.z + threadIdx.z;
	Cell<Particle>  *c0 = cells[0];
	
	
	
	
	emeElement(c0,i_s+nx,l_s+ny,k_s+nz,E,H1,H2,
	J,c1,c2,tau,
	dx1,dy1,dz1,dx2,dy2,dz2);
	}
	
	\end{lstlisting}
	\label{listing-GPU-plasma-class}
\end{ListingEnv}

\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
	\caption{Процедура, реально выполняющая вычисление электрического поля в узле сетки, реализует формулу \ref{FDTD},2 из раздела \ref{beam-plasma-methods}}
	% далее метка для ссылки:
	\label{listing-real-computer}	
	\begin{lstlisting}[language={[ISO]C++}]

	__host__ __device__                                                                                                    
	void emeElement(Cell<Particle> *c,int i,int l,int k,double *E,double *H1, double *H2,                          
	double *J,double c1,double c2, double tau,                                                     
	int dx1,int dy1,int dz1,int dx2,int dy2,int dz2                                                
	)                                                                                              
	{                                                                                                              
	int n  = c->getGlobalCellNumber(i,l,k);                                                                     
	int n1 = c->getGlobalCellNumber(i+dx1,l+dy1,k+dz1);                                                          
	int n2 = c->getGlobalCellNumber(i+dx2,l+dy2,k+dz2);                                                          
	
	E[n] += c1*(H1[n] - H1[n1]) - c2*(H2[n] - H2[n2]) - tau*J[n];                                                
	}   
	\end{lstlisting}
\end{ListingEnv}




\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
	\caption{Ядро CUDA, предназначенное для вычисления электрического поля, адаптированное под универсальный формат вызова.}
	% далее метка для ссылки:
	\label{parameter-struct1}	
	\begin{lstlisting}[language={[ISO]C++}]
	
		template <template <class Particle> class Cell >
		__device__ void GPU_eme_SingleNode(
		
		Cell<Particle>  **cells,
		KernelParams *params,
		unsigned int bk_nx,unsigned int bk_ny,unsigned int bk_nz,
		unsigned int tnx,unsigned int tny,unsigned int tnz
		)
		{
		unsigned int nx = bk_nx*params->blockDim_x + tnx;
		unsigned int ny = bk_ny*params->blockDim_y + tny;
		unsigned int nz = bk_nz*params->blockDim_z + tnz;
		Cell<Particle>  *c0 = cells[0];
		
		emeElement(c0,params->i_s+nx,params->l_s+ny,params->k_s+nz,params->E,params->H1,params->H2,
		params->J,params->c1,params->c2,params->tau,
		params->dx1,params->dy1,params->dz1,
		params->dx2,params->dy2,params->dz2);
		}
	
	\end{lstlisting}
	\label{listing-computer-universal-format}
\end{ListingEnv}

\subsection{Механизм передачи параметров расчетных процедур}


Из листинга \ref{listing-real-computer} видно, что процедура emeElement имеет некий набор параметров, более того, ясно, что у всех расчетных процедур набор параметров будет различным. Для
того, чтобы привести все такие процедуры к единому формату, так чтобы можно было передавать эти процедуры через параметр типа SingleNodeFunctionType (листинг \ref{listing-signature}), была введена структура KernelParams, включающая в себя все возможные наборы параметров для всех расчетных процедур, листинг \ref{parameter-struct}. Задача упрощается за счет того, что наборы параметров различных процедур имеют много общего. 

Образец приведения расчетной процедуры к универсальному формату показан на листинге \ref{listing-computer-universal-format}. Здесь наиболее важное отличие от \ref{Original-function} состоит в том что, что координаты обрабатываемого узла \textit{nx, ny, nz} вычисляются на основе параметров процедуры, а не на основе внутренних переменных CUDA (\textit{blockId,blockDim.x,threadIdx}).

Таким образом, перед каждым запуском расчетной процедуры (того, что прежде было запуском ядра CUDA), заполняются те поля структуры \textit{KernelParams}, которые нужны именно для данной процедуры.
Далее вызывается процедура \textit{Kernel\_Launcher}, листинг \ref{universal-launcher}, которой передаются массив всех ячеек сетки \textit{cells}, указатель на структуру, содержащую набор параметров \textit{params}, размерности сетки и блока  и вызываемая расчетная процедура \textit{h\_snf}.


\begingroup
\captiondelim{ } % разделитель идентификатора с номером от наименования
\lstinputlisting[lastline=49,language={[ISO]C++},caption={Структура, включающая в себя все возможные наборы параметров для всех расчетных процедур},label={parameter-struct}]{listings/kernel_params.cxx}
\endgroup 

\clearpage





\section{Анализ производительности узлов с многоядерными процессорами и ускорителями вычислений} 
В этом разделе представлены результаты анализа производительности узлов с многоядерными процессорами разных типов и ускорителями вычислений, построенными по технологии, совместимой с x86 (Intel Xeon Phi различных поколений)

Основные вопросы те же, что и в разделе, посвященном графическим ускорителям: возможность полноценного использования вычислительной мощности 
ускорителей типа Intel Phi без задержек на перемещение данных и возможность эффективной реализации вычислительных алгоритмов на параллельной ВС, оснащенной ускорителями такого типа. Можно привести таблицу, аналогичную таблице \ref{tab-interp-koef} с той поправкой, что эффективность многопоточного доступа к памяти на Intel Phi несколько ниже, поэтому значения интерполяционного коэффициента будут меньше для алгоритмов, использующих большой объем памяти.

В 2017, в связи с появлением в широком доступе, в том числе в ИВМиМГ, гибридных систем, в частности ВВС с узлами на основе Intel Xeon Phi, т.е. с общей памятью, становится актуальным распараллеливание также и с использованием технологии OpenMP. Такая модель распараллеливания (мелкозернистое распараллеливание) также была реализована. Были использованы директивы OpenMP в наиболее времяемком фрагменте программы, в цикле движения модельных частиц. Таким образом, речь идет об улучшении (оптимизации) мелкозернистого распараллеливания, которая заключается в подборе оптимального варианта использования средств OpenMP. 

В таблице \ref{tab-ompXeon} показано время расчета для одного, двух и четырех MPI-процессов, частицы в котором дополнительно разделены между несколькими потоками OpenMP. Здесь важно заметить, что разделение частиц между более чем четырьмя процессами над общей памятью, безусловно, возможно и средствами MPI, без привлечения дополнительных программных средств, и с той же или чуть меньшей эффективностью. Также в таблице \ref{tab-ompXeon} видно, что дальнейшее увеличение числа потоков и процессов не имеет смысла, и дальнейшее повышение скорости счета возможно лишь при использовании новых технологий. Показанные расчеты проведены на процессоре Intel Xeon X5560. Тестирование проводилось на задаче взаимодействия электронного пучка с плазмой после вхождения пучка в область, т.е. при максимальной нагрузке по частицам.

Основной целью данного раздела является создание возможности эффективного использования ускорителей Intel Xeon Phi, что невозможно иначе как с помощью OpenMP. В таблице \ref{tab-ompXeonPhi} показано время расчета на ускорителе Intel Xeon Phi  для различного количества потоков. Процессор Intel Xeon X5560 имеет 4 ядра, что обеспечивает эффективный запуск не более чем 6-8 потоков одновременно. Ускоритель Intel Xeon Phi является супер-многоядерным вычислительным устройством (в текущем варианте 72 ядра, или 288 одновременно исполняемых потоков.

\begin{center}
\begin{table} [htbp]

	\caption{Время расчета движения частиц (32 тыс.) для нескольких MPI-процессов, частицы в которых дополнительно разделены между несколькими потоками OpenMP на процессоре Intel Xeon, в секундах}
	\label{tabXeon}%
	\begin{tabular}{| c | c | c| c|}
		\hline
          &  1 MPI-процесс & 2 MPI-процесса & 4 MPI-процесса \\ \hline
1 поток   & 0.028          & 0.014          & 0.011          \\ \hline
2 потока  & 0.015          & 0.086          & 0.090           \\ \hline
4 потока  & 0.012          & 0.008          & 0.015          \\ \hline
	\end{tabular}
	\label{tab-ompXeon}
\end{table}
\end{center}

Из таблицы \ref{tabXeon} видно, что программа хорошо ускоряется на системе с общей памятью.


Здесь необходимо сделать замечание по поводу сравнения запусков на Intel  Xeon с MPI, и на Intel Xeon Phi с OpenMP, является ли корректным сравнение двух разных архитектур с двумя разными технологиями параллельного программирования. Тем не менее, такое сравнение имееет смысл, потому что в итоге значение имеет только время работы. Что касается самой методики сравнения, то несмотря на то, что программу, реализующую метод частиц в ячейках можно запускать в несколько MPI-процессов на один узел, это не имеет практического смысла, так как ускорение является почти линейным по потокам внутри процесса, и таким образом, нормальный режим запуска - один MPI-процесс на один процессор, или один ускоритель.


Необходимо отдельно охарактеризовать работу созданного тестового приложения на ускорителе вычислений Intel Xeon Phi. Вопрос заключается в том, можно ли ускоритель рассматривать в данном случае как такой же процессор, только с очень большим количеством ядер, или нужен какой-то особый подход к тестированию ВС на основе Intel Xeon Phi?

Для того, чтобы ответить на этот вопрос, были проведены аналогичные тестовые расчеты на Intel Xeon Phi. 
В свете описанного выше различия между Intel Xeon и Intel Xeon Phi (по возможному количеству запускаемых потоков) запустить и там, и там одинаковое количество потоков нельзя:
\begin{itemize}
\item 4 потока на Intel Xeon и 4 потока на Intel Xeon Phi запускать нет смысла, известно, что на Intel Xeon Phi ядра более слабые,
\item  40 потоков на Intel Xeon запустить нельзя, 
\end{itemize}
можно сравнивать только в целом, процессор Intel Xeon и ускоритель Intel Xeon Phi). Сравнение в данном случае можно проводить только по итоговому времени, результат показан в таблице \ref{tab-ompXeonPhi}. 

В таблице видно, что ускорение в зависимости от числа потоков присутствует так же как и на процессоре Intel Xeon, и также близко к линейному, из чего следует, что в рамках тестового приложения ускоритель Intel Xeon Phi может рассматриваться как процессор с очень большим числом ядер.

\begin{center}
\begin{table} [htbp]
\caption{Время расчета движения частиц (32 тыс.) на ускорителе Intel Xeon Phi.}
\label{tabXeonPhi}%
\begin{tabular}{| c | c |}
\hline
		            & время, сек.  \\ \hline
		10 потоков  & 0.0078    \\ \hline
		20 потоков  & 0.0022      \\ \hline
		40 потоков  & 0.0013        \\ \hline
\end{tabular}
\label{tab-ompXeonPhi}
\end{table}
\end{center}


Выводы по второму разделу: проведена оптимизация мелкозернистого распараллеливания, показана возможность повышения скорости расчета разработанного кода с использованием ускорителей Intel Xeon Phi. 0.01 сек. в таблице (Intel Xeon) против 0.001 сек. на графике (Intel Xeon Phi) т.е. на порядок.
Подбор оптимального набора директив OpenMP и опций компилятора

Проведенная в данном разделе работа преследует две цели: повысить скорость работы без изменения кода и снять ограничения на размер массивов (частиц и сетки), которые накладывает использование OpenMP. В таблице \ref{tabOMPoptions} показаны использованные опции и результат подбора, а именно время работы наиболее затратной части (движения модельных частиц).  Оптимизация проводилась на задаче взаимодействия электронного пуска с плазмой после вхождения пучка в область. Цель проведения оптимизации: с максимальной эффективностью задействовать ускорители Intel Xeon Phi, являющиеся в данный момент наиболее эффективным (из доступных) инструментов решения вычислительных задач.

\begin{center}
\begin{table} [htbp]
\caption{Время работы (в мс) процедуры расчета движения модельных частиц.}\label{tabOMPoptions}%
\begin{tabular}{| c | c |}
\hline
Основной вариант (-О1 включен по умолчанию)   &  155.3 \\ \hline
-O2 &  80.01   \\ \hline
-O3 &  128.029   \\ \hline
O2 -xCORE-AVX2 & 76 \\ 
\hline
\end{tabular}
\end{table}
\end{center}


Наиболее оптимальным является сочетание опций -O2 -xCORE-AVX2 (для Intel Xeon Phi эта опция имеет вид -xCORE-AVX512).

На основании данных, приведенных в этом разделе, а именно того, что компиляция с различными опциями не меняет принципиально время счета, можно \textbf{сделать следующий вывод}: возможность тестирования ВС на основе метода частиц не зависит от тонких настроек компилятора, операционной системы и пр.  

