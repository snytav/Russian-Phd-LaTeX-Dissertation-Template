\chapter{Анализ производительности узлов мультиархитектурной ВС} \label{chapt3}
В \textbf{третьей главе} описаны методы, позволяющие определять скорость счета на ускорителях вычислений и скорость перемещения данных между ускорителем вычислений и хост-машиной, а также давать прогнозы о скорости счета нереализованных еще алгоритмов на тестируемой ВС.

Кроме того, предожена методика оценки качества узлов мультиархитектурной ВС на основе графических ( или других) ускорителей, при этом качество понимается как сбалнсированность средней оценочной скорости счета на ускорителе и скорости пермещения данныхмежду ускорителем и хостом. 

\section{Анализ производительности узлов с графическими ускорителями} \label{sect3_1}

\begin{table}[ht]
	\begin{center}
		\caption{Основные параметры трех GPU, использованных в оптимизационных тестах}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Название                &  GeForce GTX 850M & Tesla M2090 & Tesla K40m \\ \hline
			Объем                  &                   &             &             \\
			глобальной памяти       & 4096 MB           & 5375 MB     & 11520 MB \\ \hline
			Количество              &             &               &     \\
			мультипроцессоров       & 5           &  16           & 15  \\ \hline
			Количество              &             &               &     \\
			ядер CUDA               & 640         & 512         & 2880  \\ \hline
			Тактовая частота        &             &             &         \\
			GPU                     & 902 MHz     & 1301 MHz    & 745 MHz \\ \hline
			Ширина                  &             &             &         \\
			шины памяти             & 128-bit     & 384-bit     & 384-bit \\ \hline
		\end{tabular}
		\label{GPUs}
	\end{center}
\end{table}


В таблице \ref {PerfGPUs} приведены времена для различных частей алгоритма на трех разных GPU, параметры которых приведены в таблице \ref{GPUs}. Здесь важны столько сами числа, но и возможность проанализировать различия и предложить новую стратегию оптимизации. Во-первых, видно, что вычисление полей работает быстрее с K40. Это означает, что регулярный доступ к памяти должен быть обеспечен везде, чтобы достигнуть высокой производительности. Во-вторых, сдвиг частиц выполняется почти в 10 раз медленнее на  GeForce, у которого больше ядер, чем у Tesla2090. Это означает, что программа зависит, главным образом, от ширины шины памяти
и числа мультипроцессоров. В-третьих, времена присваивания токов  близки для Tesla и Kepler (Tesla K40). Это означает, что не все ядра используются (на графическом ускорителе Kepler их в 5 раз больше!).

В таблице \ref{tabP100} приведено время работы основных частей алгоритма на Tesla K40, Tesla K80 (использован 1 GPU) по сравнению с P100.

Обращает на себя внимание тот факт, что движение модельных частиц на P100 вычисляется даже немного медленнее, чем на K80, впрочем, разницу (10 мкс) можно отнести на счет погрешности измерений. В то же время значительно ускорен расчет магнитного поля и, что наиболее важно, накладные расходы, связанные с переупорядочиванием модельных частиц, стали намного меньше. Это означает, что  переход на P100 может значительно ускорить расчеты с использованием метода частиц.

\begin{table}[ht]
	\caption{ Основные части вычислительного алгоритма на различных GPU, в миллисекундах.}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Название GPU & K40 & K80 & P100\\\hline
			Вычисление электрического поля      & 6    & 4.6     & 2.5    \\\hline
			Движение частиц                     &10500 & 294.24  & 306.9  \\\hline
			Вычисление магнитного поля (1 этап) & 190  & 67.3    & 14.7   \\\hline
			Вычисление магнитного поля (2 этап) & 112  & 40.277  & 10.4   \\ \hline
			Переупорядочивание частиц (1 этап)  & 16   & 33.72   & 10.24  \\ \hline
			Переупорядочивание частиц (2 этап)  & 1.13 & 444.7   & 81.6       \\ \hline
		\end{tabular}
	\end{center}
	\label{tabP100}
\end{table}


Основной вопрос данного раздела, как и всей работы - что можно узнать о данной ВС путем запуска программы, реализующей метод частиц в ячейках? В отличие от 	большинства других разделов информация о характеристиках оборудования в данном случае доступна через стандартный интерфейс, соответственно фактически измеренную скорость счета и скорость пересылки данных между хостом и GPU можно сравнивать с номинальными показателями.

Аналогично разделу \ref{calc_PE} определяется производительность GPU во флопсах как для этапу расчета частиц, так и для этапа расчета электромагнитного поля

Важнейшей интегральной характеристикой ВС, оснащенной графическими ускорителями, является возможность их полноценно использовать. Эта возможность
может быть измерена с помощью сопоставления вычисленной скорости счета и и скорости пересылок данных между хостом и GPU с использованием описанного 
в разделе \ref{complex_estimate} переводного множителя $k_{f2b}$. Этот множитель отражает принципиальную возможность переслать необходимые данные с GPU на хост и далее по коммуникационной сети ВС на соседние узлы раньше, чем они понадобятся для счета на соседнем узле, и таким образом счет может продолжать без задержек, вызванных комммуникациями.

Вместе с тем вопрос, который наиболее часто задают специалисты по математическому моделированию применительно к мультиархитектурной ВС, оснащенной графическими ускорителями - это возможность \textit{эффективной} реализации конкретного вычислительного алгоритма на даннной мультиархитектурной ВС.
Для ответа на данный вопрос предлагается интерполяционная формула:
\begin{equation}
v_{pre} = v_{PIC} k + (1-k) v_{B,E}
\end{equation} 
здесь $ v_{pre}$ - оценка скорости вычислений на GPU для рассматриваемого алгоритма, $v_{PIC}$ - скорость вычислений с на этапе сдвига модельных частиц, $v_{B,E}$ - на этапе расчета электромагнитного поля, а $k$ - интерполяционный множитель, получаемый из следующих соображений.

Как уже говорилось выше, большинство численных методов используемых в математическом моделировании находятся в промежуточном положении по отношению к используемым в методе частиц в ячейках алгоритму вычисления поля и алгоритму расчета движения частиц по следующим показателям:
\begin{itemize}
	\item вычислительной интенсивности (равномерное распределение вычислительно сложных фрагментов по тексту или отдельные высоконагруженные участки);
	\item характеру доступа к оперативнной памяти (регулярный или нерегулярный);
	\item объему используемых данных (большой или маленький).
\end{itemize}
Ориентировочное распределение вычислительных алгоритмов по рассмотренным показателям и соответствующие значения коэффициента $k$ показаны в таблице 


\begin{table}[ht]
	\begin{center}
		\caption{Определение интерполяционного коэффициента для некоторых типов вычислительных алгоритмов}
		\begin{tabular}{|c|c|c|c|c|}
			%	        &   &  &  & k \\ \hline
			\hline
			Вычислительный & Интенсивность &  Доступ к     & Объем  & $k$  \\ 
		    	алгоритм   &               &   оперативной   & данных&  \\
		                   &               &   памяти        &       &  \\ \hline
			
			
			Расчет движения  &  низкая & нерегулярный & большой &
			1.0 \\ 
			модельных частиц                            &         &             &          & \\\hline
			Метод Монте-Карло                &  низкая & нерегулярный & средний & 0.9 \\ \hline
			Метод SPH    &  низкая & нерегулярный & небольшой & 0.6 \\ \hline
			Метод              &  высокая & нерегулярный & большой & 0.5  \\
			конечных элементов &          &              &         & \\ \hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.2 \\ 		
			схемы (явные)      &           &            &         &     \\\hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.1 \\ 		
			схемы (явные)-2    &           &            &         &     \\\hline
			
			Вычисление         &  высокая  & регулярный & большой & 0.0 \\ 		
			электромагнитного поля      &           &            &         &     \\\hline
			
			
		\end{tabular} 
		\label{tab-interp-koef}              
	\end{center}
\end{table}

\section{Механизм реализации переносимой программы}

Технология переноса программ численного моделирования с GPU на Intel Xeon Phi
будет показана на примере программы для моделирования динамики плазмы методом частиц в ячейках.

Вначале необходимо ответить на вопрос, для чего нужна такая методика?

Во-первых, необходимо иметь возможность использовать наиболее мощные гибридные суперЭВМ, а такие сейчас строятся (в том числе) на базе Intel Xeon Phi. Кроме того, в
докладе А.О.Лациса “Что же делать с этим многообразием суперкомпьютерных миров?” на конференции “Научный сервис в сети Интернет-2014” \cite{Lacis2014} была предложена методика создания единого переносимого программного обеспечения для решения вычислительных задач, которое могло бы использоваться на многих суперкомпьтерных архитектурах. Эта методика основана на использовании библиотеки BLAS, которая так или иначе существует на всех машинах.

Большое количество различных суперкомпьютерных архитектур приводит к необходимости разрабатывать отдельный вариант программы под каждую из них.
В то же время наиболее распространенные в настоящее время суперкомпьютерные архитектуры строятся на основе одних и тех же принципов, т.е. кластеры с использованием ускорителей вычислений или просто кластеры.
Это означает, что задача создания инструмента для облегченного (упрощенного), хотя и не автоматического перехода между двумя разными  суперкомпьютерными архитектурами
представляется осуществимой.

Вопросы портирования программ на Intel Xeon Phi, в частности, рассматриваются в \cite{Rosales2Phi}. Кроме того, в работе \cite{Nakashima2015} изучается проблема достижения максимальной заявленной производительности в 1 Teraflops с помощью ускорителя Intel Xeon Phi. Сравнение производительности ускорителей Intel Xeon Phi и графических ускорителей на различных задачах проведено в \cite{Lyakh201584,Liu2015230,Bernaschi20142495}.

Новизна методики переноса, созданной в диссертации заключается в разработке полуавтоматического средства переноса программ,
которое с одной стороны, было бы эффективным,
с другой стороны, обеспечивало бы полный контроль над процессом переноса для прикладного программиста.

\textbf{Постановка задачи}

Необходимо решить вопрос о переносе между наиболее распространенными типами суперкомпьютерных архитектур
\begin{enumerate}
	\item Кластера на основе Nvidia Kepler 
	\item Кластера на основе Intel Xeon Phi
	\item Кластера на основе Intel Xeon или Sun UltraSPARC
\end{enumerate}
Рассматривается перенос программы с GPU на Intel Xeon Phi (не наоборот!!!) и не рассматривается на данный момент вопрос оптимизации под ту или иную архитектуру.

Основные проблемы переноса с архитектуры CUDA\cite{CUDAweb,Boreskov,Sanders} на архитектуру MIC\cite{MorganPhi,FangPhi2014}
\begin{enumerate}
	\item Компиляция ядер CUDA 
	и в особенности вызовов ядер CUDA без компилятора Nvidia
	\item Пропуск операций копирования между различными видами памяти в CUDA
	\item Определение типов данных и ключевых слов, входящих в расширение языка C, используемое в CUDA.
\end{enumerate}


\begin{itemize}
	\item Архитектурно-зависимые участки кода 
	\begin{itemize}
		\item Сводятся к минимуму
		\item Оформляются в виде процедур 
		\item Выносятся во внешнюю подключаемую библиотеку
	\end{itemize}
	\item Таким образом в тексте программы присутствует некий обобщенный вызов процедуры, который приобретает конкретную форму при компиляции в зависимости
	\begin{itemize}
		\item От компилятора
		\item От архитектуры
	\end{itemize}
\end{itemize}

Сведение к минимуму архитектурно-зависимых участков кода выполняется следующим образом 
В коде имеется 15-20 вызовов небольших вычислительных процедур, 
выполняющих обработку:
\begin{itemize}
	\item узлов сетки
	\item модельных частиц
	\item границ расчетной области
\end{itemize}
Эти процедуры оформлены в виде ядер CUDA. Таким образом, эти процедуры не могут быть скомпилированы с помощью компилятора Intel и пр.
Основной принцип предлагаемой методики переноса программ: \textbf{сделать такой участок кода по крайней мере единственным}.

\textbf{Описание методики переноса программ}
\textbf{Универсальная процедура запуска}

В качестве реализации сформулированного выше принципа (вынести все непереносимые элементы кода в одну процедуру) предлагается универсальная процедура запуска, которая показана на рисунке \ref{universal-launcher}. На вход этой процедуре в качестве параметра подаются процедуры, которые запускаются из-под ядер CUDA. Далее в том случае, если этот код компилируется компилятором CUDA C/C++ и исполняется на машине с GPU, то процедурный параметр передается универсальному ядру (GPU\_Universal\_Kernel на рисунке) и запускается на GPU внутри ядра. Если же этот код компилируется компилятором Intel (например) и исполняется
на ускорителе Intel Xeon Phi (в режиме native) или просто на многоядерном процессоре под OpenMP, то переданная в качестве параметра процедура будет просто вызываться в цикле. Цикл в данном случае имеет шестикратную степень вложенности (соответствующую шести размерностям сетки потоковых блоков, используемой в CUDA - три размерности сетки и три размерности потокового блока).
\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
	\caption{Универсальная процедура запуска}
	\label{universal-launcher}	
	\begin{lstlisting}[language={[ISO]C++}]
	
	int Kernel_Launcher(
	Cell<Particle>  **cells,KernelParams *params,
	unsigned int grid_size_x,unsigned int grid_size_y,unsigned int grid_size_z,
	unsigned int block_size_x,unsigned int block_size_y,unsigned int block_size_z,
	int shmem_size,
	SingleNodeFunctionType h_snf,char *name)
	{
	struct timeval tv1,tv2;
	#ifdef __CUDACC__
	dim3 blocks(grid_size_x,grid_size_y,grid_size_z),threads(block_size_x,block_size_y,block_size_z);
	
	gettimeofday(&tv1,NULL);
	GPU_Universal_Kernel<<<blocks,threads,shmem_size>>>(cells,params,h_snf);
	DeviceSynchronize();
	gettimeofday(&tv2,NULL);
	#else
	char hostname[1000];
	gethostname(hostname,1000);
	
	#ifdef OMP_OUTPUT
	printf("function %s executed on %s \n",name,hostname);
	#endif
	
	gettimeofday(&tv1,NULL);
	
	omp_set_num_threads(OMP_NUM_THREADS);
	
	#pragma omp parallel for
	for(int i = 0;i < grid_size_x;i++)
	{
	//      ....              
	h_snf(cells,params,i,j,k,i1,j1,k1);
	// ....
	}
	}
	\end{lstlisting}
\end{ListingEnv}

\textbf{Унифицированная сигнатура расчетных процедур}
Все расчетные процедуры, которые ранее запускались как ядра CUDA, должны быть оформлены в виде процедур с единой сигнатурой (одинаковый тип возвращаемого значения и одинаковый набор параметров), показанной на рисунке \ref{listing-signature}.

Далее необходимо отработать расширения языка C, используемые в CUDA С/С++, как-то специальные типы данных
(int3, double3, dim3, и пр.). Их можно либо доопределить, либо при возможности скопировать файл cuda.h. Специальные ключевые слова: \_\_global\_\_ ,         \_\_device\_\_, и др. можно 
замаскировать с помощью директив условной компиляции.

Остаются функции CUDA API: копирование из одного типа памяти в другой, обработка ошибок и пр. Эти функции должны вызываться не напрямую, как CUDA API, а через функции-обертки, вынесенные в отдельный заголовочный файл.

\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
\caption{Тип универсальной счетной процедуры}
\label{listing-signature}
	\begin{lstlisting}[language={[ISO]C++}]
	typedef void (*SingleNodeFunctionType)(GPUCell<Particle>  **cells,KernelParams *params,
	unsigned int bk_nx,unsigned int bk_ny,unsigned int bk_nz,
	unsigned int nx,unsigned int ny,unsigned int nz
	);
	
	\end{lstlisting}
\end{ListingEnv}

Этот процесс будет показан на примере процедуры расчета электрического поля из класса GPU-пространство моделирования \ref{listing-GPU-plasma-class}. На листинге \ref{Original-function} показано первоначально имеющееся ядро CUDA, предназначенное для вычисления электрического поля в одном узле сетки. При этом реальные вычисления проводятся процедурой emeElement. Само ядро только лишь определяет узел сетки с помощью внутренних индексов CUDA, передает параметры (шаги сетки, временной шаг, магнитное поле, соответсвующую компоненту тока, как описано в разделе \ref{beam-plasma-methods}) и вызывает процедуру emeElement.


\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
\caption{Первоначально имеющееся ядро CUDA, предназначенное для вычисления электрического поля.}
\label{Original-function}	
	\begin{lstlisting}[language={[ISO]C++}]
	template <template <class Particle> class Cell >
	__global__ void GPU_eme(
	
	Cell<Particle>  **cells,
	int i_s,int l_s,int k_s,
	double *E,double *H1, double *H2,
	double *J,double c1,double c2, double tau,
	int dx1,int dy1,int dz1,int dx2,int dy2,int dz2
	)
	{
	unsigned int nx = blockIdx.x*blockDim.x + threadIdx.x;
	unsigned int ny = blockIdx.y*blockDim.y + threadIdx.y;
	unsigned int nz = blockIdx.z*blockDim.z + threadIdx.z;
	Cell<Particle>  *c0 = cells[0];
	
	
	
	
	emeElement(c0,i_s+nx,l_s+ny,k_s+nz,E,H1,H2,
	J,c1,c2,tau,
	dx1,dy1,dz1,dx2,dy2,dz2);
	}
	
	\end{lstlisting}
\end{ListingEnv}

\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
	\caption{Процедура, реально выполняющая вычисление электрического поля в узле сетки, реализует формулу \ref{FDTD},2 из раздела \ref{beam-plasma-methods}}
	% далее метка для ссылки:
	\label{listing-real-computer}	
	\begin{lstlisting}[language={[ISO]C++}]

	__host__ __device__                                                                                                    
	void emeElement(Cell<Particle> *c,int i,int l,int k,double *E,double *H1, double *H2,                          
	double *J,double c1,double c2, double tau,                                                     
	int dx1,int dy1,int dz1,int dx2,int dy2,int dz2                                                
	)                                                                                              
	{                                                                                                              
	int n  = c->getGlobalCellNumber(i,l,k);                                                                     
	int n1 = c->getGlobalCellNumber(i+dx1,l+dy1,k+dz1);                                                          
	int n2 = c->getGlobalCellNumber(i+dx2,l+dy2,k+dz2);                                                          
	
	E[n] += c1*(H1[n] - H1[n1]) - c2*(H2[n] - H2[n2]) - tau*J[n];                                                
	}   
	\end{lstlisting}
\end{ListingEnv}




\begin{ListingEnv}[!h]
	\captiondelim{ } % разделитель идентификатора с номером от наименования
	\caption{Ядро CUDA, предназначенное для вычисления электрического поля, адаптированное под универсальный формат вызова.}
	% далее метка для ссылки:
	\label{parameter-struct1}	
	\begin{lstlisting}[language={[ISO]C++}]
	
		template <template <class Particle> class Cell >
		__device__ void GPU_eme_SingleNode(
		
		Cell<Particle>  **cells,
		KernelParams *params,
		unsigned int bk_nx,unsigned int bk_ny,unsigned int bk_nz,
		unsigned int tnx,unsigned int tny,unsigned int tnz
		)
		{
		unsigned int nx = bk_nx*params->blockDim_x + tnx;
		unsigned int ny = bk_ny*params->blockDim_y + tny;
		unsigned int nz = bk_nz*params->blockDim_z + tnz;
		Cell<Particle>  *c0 = cells[0];
		
		emeElement(c0,params->i_s+nx,params->l_s+ny,params->k_s+nz,params->E,params->H1,params->H2,
		params->J,params->c1,params->c2,params->tau,
		params->dx1,params->dy1,params->dz1,
		params->dx2,params->dy2,params->dz2);
		}
	
	\end{lstlisting}
\end{ListingEnv}

\textbf{Механизм передачи параметров расчетных процедур}


Из рисунка \ref{listing-real-computer} видно, что процедура emeElement имеет некий набор параметров, более того, ясно, что у всех расчетных процедур набор параметров будет различным. Для
того, чтобы привести все такие процедуры к единому формату, так чтобы можно было передавать эти процедуры через параметр типа SingleNodeFunctionType (рисунок \ref{listing-signature}), была введена структура KernelParams, включающая в себя все возможные наборы параметров для всех расчетных процедур \ref{parameter-struct}. Задача упрощается за счет того, что наборы параметров различных процедур имеют много общего. 

Образец приведения расчетной процедуры к универсальному формату показан на рисунке \ref{listing-computer-universal-format}. Здесь наиболее важное отличие от \ref{Original-function} состоит в том что, что координаты обрабатываемого узла \textit{nx,ny,nz} вычисляются на основе параметров процедуры, а не на основе внутренних переменных CUDA (\textit{blockId,blockDim.x,threadIdx}).

Таким образом, перед каждым запуском расчетной процедуры (того, что прежде было запуском ядра CUDA), заполняются те поля структуры \textit{KernelParams}, которые нужны именно для данной процедуры.
Далее вызывается процедура \textit{Kernel\_Launcher}, рисунок \ref{universal-launcher}, которой передаются массив всех ячеек сетки \textit{cells}, указатель на структуру, содержащую набор параметров \textit{params}, размерности сетки и блока  и вызываемая расчетная процедура \textit{h\_snf}.

\begin{ListingEnv}[!h]
\captiondelim{ } % разделитель идентификатора с номером от наименования
\caption{Структура, включающая в себя все возможные наборы параметров для всех расчетных процедур}
% далее метка для ссылки:
\label{parameter-struct}	
 \begin{lstlisting}[language={[ISO]C++}]
	typedef struct {
	double d_ee; //electric energy
	double *d_Ex,*d_Ey,*d_Ez; // electric field
	double *d_Hx,*d_Hy,*d_Hz; // magnetic field
	double *d_Jx,*d_Jy,*d_Jz; // currents
	double *d_Rho;
	int nt;                                 // timestep
	int *d_stage;                           // checking system (e.g. for flow-out particles)
	int *numbers;                            // number of particles in each cell
	double mass,q_mass;
	double *d_ctrlParticles;
	int jmp;
	//	                                        for periodical FIELDS
	int i_s,k_s;                        //
	double *E;                              //the field
	int dir;                                // the direction being processed
	int to,from;                            // the range along the direction
	
	//	                                        for periodical CURRENTS
	int dirE;                           // directions
	int N;                           // variables
	
	//                                          electric field solver
	int l_s;                        // variables
	double *H1,*H2;                          // magnetic fields (orthogonal)
	double *J;                              // current
	double c1,c2,tau;                       //grid steps squared
	int dx1,dy1,dz1,dx2,dy2,dz2;            //shifts
	
	//	                                        magnetic field solver
	double *Q;                              // magnetic field at half-step
	double *H;                              // magnetic field
	double *E1,*E2;                         // electric fields (orthogonal)
	int particles_processed_by_a_single_thread;
	unsigned int blockDim_x,blockDim_y,blockDim_z; // block for field solver
	} KernelParams;
	 \end{lstlisting}
	\end{ListingEnv}
%	\caption{Структура, включающая в себя все возможные наборы параметров для всех расчетных процедур}
%	\label{parameter-struct}
%\end{figure}
\clearpage





\section{Анализ производительности узлов с многоядерными процессорами и ускорителями вычислений} 
В этом представлены результаты анализа производительности узлов с многоядерными процессорами разных типов и ускорителями вычислений, построенными по технологии, совместимой с x86 (Intel Xeon Phi различных поколений)

Основные вопросы те же, что и разделе, посвященном графическим ускорителям: возможность полноценного использования вычислительной мощности 
ускорителей типа Intel Phi без задержек на перемещение данных и возможность эффективной реализации вычислительных алгоритмов на параллельной ВС, оснащенной ускорителми такого типа. Можно привести таблицу, аналогичную \ref{tab-interp-koef} с той поправкой, что эффективность многопоточного доступа к памяти на Intel Phi несколько ниже, поэтому значения интерполяционого коэффициента будут меньше для алгоритмов, использующих большой объем памяти.

В 2017, в связи с появлением в широком доступе, в том числе в ИВМиМГ гибридных систем, т.е. суперЭВМ с узлами на основе Intel Xeon Phi, т.е. с общей памятью, становится актуальным распараллеливание также и с использованием технологии OpenMP. Такая модель распараллеливания (мелкозернистое распараллеливание) также была реализована. (Подробности – использованы директивы OpenMP в наиболее времяемком фрагменте программы, в цикле движения модельных частиц) Таким образом, речь идет об улучшении (оптимизации) мелкозернистого распараллеливания, которая заключается в подборе оптимального варианта использования средств OpenMP. 
В таблице 1 показано время расчета для одного, двух и четырех MPI-процессов, частицы в котором дополнительно разделены между несколькими потоками OpenMP. Здесь важно заметить, что разделение частиц между более чем четырьмя процессами над общей памятью, безусловно, возможно и средствами MPI, без привлечения дополнительных программных средств, и с той же или чуть меньшей эффективностью. Также в таблицу \ref{tabXeon} видно, что дальнейшее увеличение числа потоков и процессов не имеет смысла, и дальнейшее повышение скорости счета возможно лишь при использовании новых технологий. Показанные расчеты проведены на процессоре Intel Xeon X5560. Тестирование проводилось на задаче взаимодействия электронного пуска с плазмой после вхождения пучка в область, т.е. при максимальной нагрузке по частицам.
Основной целью данного раздела является создание возможности эффективного использования ускорителей Intel Xeon Phi, что невозможно иначе как с помощью OpenMP. На рисунке 6 показано время расчета на ускорителе Intel Xeon Phi  для различного количества потоков. Процессор Intel Xeon X5560 имеет 4 ядра, что обеспечивает эффективный запуск не более чем 6-8 потоков одновременно. Ускоритель Intel Xeon Phi является супер-многоядерным вычислительным устройством (в текущем варианте 72 ядра, или 288 одновременно исполняемых потоков

Таблица 2. Время расчета движения частиц (32 тыс.) для нескольких MPI-процессов, частицы в которых дополнительно разделены между несколькими потоками OpenMP на процессоре Intel Xeon.

\begin{table} [htbp]
	\centering
	\changecaptionwidth\captionwidth{15cm}
	\caption{Время расчета движения частиц (32 тыс.) для нескольких MPI-процессов, частицы в которых дополнительно разделены между несколькими потоками OpenMP на процессоре Intel Xeon}
	\label{tabXeon}%
	\begin{tabular}{| c | c | c| c|}
		\hline
          &  1 MPI-процесс & 2 MPI-процесса & 4 MPI-процесса \\ \hline
1 поток   & 0.028 сек.     & 0.014          & 0.011          \\ \hline
2 потока  & 0.01515        & 0.086          & 0.09           \\ \hline
4 потока  & 0.012          & 0.008          & 0.015          \\ \hline
	\end{tabular}
\end{table}

Из таблицы \ref{tabXeon} видно, что несмотря на то, что программу, реализующую метод частиц в ячейках можно запускать в несколько MPI-процессов на один узел, это не имеет практического смысла, так как ускорени является почти линейным по потокам внутри процесса, и таким образом, нормальный режим запуска - один MPI-процесс на один процессор, или один ускоритель.


Необходимо отдельно охарактеризовать работу созданного тестового приложения на ускорителе вычислений Intel Xeon Phi. Вопрос заключается в том, можно ли ускоритель рассматривать в данном случае как такой же процессор, только с очень большим количеством ядер, или нужен какой-то особый подход к тестированию ВС на основе Intel Xeon Phi?

Для того, чтобы ответить на этот вопрос, были проведены аналогичные тестовые расчеты на Intel Xeon Phiю 
В свете описанного выше различия между Intel Xeon и Intel Xeon Phi (по возможному количеству запускаемых потоков) запустить и там, и там однинаковое количество потоков нельзя (4 потока на Intel Xeon и 4 потока на Intel Xeon Phi – нет смысла, известно, что на Intel Xeon Phi ядра более слабые, 40 потоков на Intel Xeon запустить нельзя, можно сравнивать только в целом, процессор Intel Xeon и ускоритель Intel Xeon Phi). Сравнение в данном случае можно проводить только по итоговому времени, результат показан в таблице \ref{tabXeonPhi}. 

В таблице видно, что ускорение в зависимости от числа потоков присутствует так же как и на процессоре Intel Xeon, и также близко к линейному, из чего следует, что в рамках тестового приложения ускоритель Intel Xeon Phi может рассматриваться как процессор с очень большим числом ядер.


\begin{table} [htbp]
	\centering
	\changecaptionwidth\captionwidth{15cm}
	\caption{Время расчета движения частиц (32 тыс.) на ускорителе Intel Xeon Phi.}
	\label{tabXeon}%
	\begin{tabular}{| c | c |}
		\hline
		            & время, сек.  \\ \hline
		10 потоков  & 0.0078    \\ \hline
		20 потоков  & 0.0022      \\ \hline
		40 потоков  & 0.0013        \\ \hline
	\end{tabular}
\end{table}


Выводы по второму разделу: проведена оптимизация мелкозернистого распараллеливания, показана возможность повышения скорости расчета разработанного кода с использованием ускорителей Intel Xeon Phi. 0.01 сек. в таблице (Intel Xeon) против 0.001 сек. на графике (Intel Xeon Phi) т.е. на порядок.
Подбор оптимального набора директив OpenMP и опций компилятора

Проведенная в данном разделе работа преследует две цели: повысить скорость работы без изменения кода и снять ограничения на размер массивов (частиц и сетки), которые накладывает использование OpenMP. В таблице \ref{tabOMPoptions} показаны использованные опции и результат подбора, а именно время работы наиболее затратной части (движения модельных частиц).  Оптимизация проводилась на задаче взаимодействия электронного пуска с плазмой после вхождения пучка в область. Цель проведения оптимизации: с максимальной эффективностью задействовать ускорители Intel Xeon Phi, являющиеся в данный момент наиболее эффективным (из доступных) инструментов решения вычислительных задач.

\begin{table} [htbp]
	\centering
	\changecaptionwidth\captionwidth{15cm}
	\caption{Время работы (в мс) процедуры расчета движения модельных частиц.}\label{tabOMPoptions}%
	\begin{tabular}{| c | c |}
		\hline
	
		Основной вариант (-О1 включен по умолчанию)   &  155.3 \\ \hline
		-O2 &  80.01   \\ \hline
		-O3 &  128.029   \\ \hline
		O2 -xCORE-AVX2 & 76 \\ 
		\hline
	\end{tabular}
\end{table}


Наиболее оптимальным является сочетание опций -O2 -xCORE-AVX2 (для Intel Xeon Phi эта опция имеет вид -xCORE-AVX512).

На основании данных, приведенных в этом разделе, а именно того, что копиляция с различными опциями не меняет принципиально время счета, можно сделать следующий вывод: возможность тестирования ВС на основе метода частиц не зависит от тонких настроек компилятора, операционной системы и пр.  

