\chapter{Анализ производительности узлов мультиархитектурной ВС} \label{chapt3}
В \textbf{третьей главе} описаны методы, позволяющие определять скорость счета на ускорителях вычислений и скорость перемещения данных между ускорителем вычислений и хост-машиной, а также давать прогнозы о скорости счета нереализованных еще алгоритмов на тестируемой ВС.

Кроме того, предожена методика оценки качества узлов мультиархитектурной ВС на основе графических ( или других) ускорителей, при этом качество понимается как сбалнсированность средней оценочной скорости счета на ускорителе и скорости пермещения данныхмежду ускоритлем и хостом. 

\section{Анализ производительности узлов с графическмим ускорителями} \label{sect3_1}

СРАВНИТЕЛЬНУЮ ТАБЛИЦУ РАБОТЫ НА ВСЕХ НЕДАВНИХ GPU -ПО профиляМ ЛЯМ ЛЯМ!!!

Основной вопрос данного раздела, как и всей работы - что можно узнать о данной ВС путем запуска программы, реализующей метод частиц в ячейках? В отличие от 	большинства других разделов информация о характеристиках оборудования в данном случае доступна через стандартный интерфейс, соответственно фактически измеренную скорость счета и скорость пересылки данных между хостом и GPU можно сравнивать с номинальными показателями.

Аналогично разделу \ref{calc_PE} определяется производительность GPU во флопсах как для этапу расчета частиц, так и для этапа расчета электромагнитного поля

Важнейшей интегральной характеристикой ВС, оснащенной графическими ускорителями, является возможность их полноценно использовать. Эта возможность
может быть измерена с помощью сопоставления вычисленной скорости счета и и скорости пересылок данных между хостом и GPU с использованием описанного 
в разделе \ref{complex_estimate} переводного множителя $k_{f2b}$. Этот множитель отражает принципиальную возможность переслать необходимые данные с GPU на хост и далее по коммуникационной сети ВС на соседние узлы раньше, чем они понадобятся для счета на соседнем узле, и таким образом счет может продолжать без задержек, вызванных комммуникациями.

Вместе с тем вопрос, который наиболее часто задают специалисты по математическому моделированию применительно к мультиархитектурной ВС, оснащенной графическими ускорителями - это возможность \textit{эффективной} реализации конкретного вычислительного алгоритма на даннной мультиархитектурной ВС.
Для ответа на данный вопрос предлагается интерполяционная формула:
\begin{equation}
v_{pre} = v_{PIC} k + (1-k) v_{B,E}
\end{equation} 
здесь $ v_{pre}$ - оценка скорости вычислений на GPU для рассматриваемого алгоритма, $v_{PIC}$ - скорость вычислений с на этапе сдвига модельных частиц, $v_{B,E}$ - на этапе расчета электромагнитного поля, а $k$ - интерполяционный множитель, получаемый из следующих соображений.

Как уже говорилось выше, большинство численных методов используемых в математическом моделировании находятся в промежуточном положении по отношению к используемым в методе частиц в ячейках алгоритму вычисления поля и алгоритму расчета движения частиц по следующим показателям:
\begin{itemize}
	\item вычислительной интенсивности (равномерное распределение вычислительно сложных фрагментов по тексту или отдельные высоконагруженные участки);
	\item характеру доступа к оперативнной памяти (регулярный или нерегулярный);
	\item объему используемых данных (большой или маленький).
\end{itemize}
Ориентировочное распределение вычислительных алгоритмов по рассмотренным показателям и соответствующие значения коэффициента $k$ показаны в таблице 


\begin{table}[ht]
	\begin{center}
		\caption{Определение интерполяционного коэффициента для некоторых типов вычислительных алгоритмов}
		\begin{tabular}{|c|c|c|c|c|}
			%	        &   &  &  & k \\ \hline
			\hline
			Вычислительный & Интенсивность &  Доступ к           & Объем  & $k$  \\ 
			алгоритм     &               &  оперативной памяти & данных &  \\ \hline
			
			
			Расчет движения  &  низкая & нерегулярный & большой &
			1.0 \\ 
			модельных частиц                            &         &             &          & \\\hline
			Метод Монте-Карло                &  низкая & нерегулярный & средний & 0.9 \\ \hline
			Метод SPH    &  низкая & нерегулярный & небольшой & 0.6 \\ \hline
			Метод              &  высокая & нерегулярный & большой & 0.5  \\
			конечных элементов &          &              &         & \\ \hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.2 \\ 		
			схемы (явные)      &           &            &         &     \\\hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.1 \\ 		
			схемы (явные)-2    &           &            &         &     \\\hline
			
			Вычисление         &  высокая  & регулярный & большой & 0.0 \\ 		
			электромагнитного поля      &           &            &         &     \\\hline
			
			
		\end{tabular} 
		\label{tab-interp-koef}              
	\end{center}
\end{table}

\section{Анализ производительности узлов с многоядерными процессорами и ускорителями вычислений} 
В этом представлены результаты анализа производительности узлов с многоядерными процессорами разных типов и ускорителями вычислений, построенными по технологии, совместимой с x86 (Intel Xeon Phi различных поколений)

Основные вопросы те же, что и разделе, посвященном графическим ускорителям: возможность полноценного использования вычислительной мощности 
ускорителей типа Intel Phi без задержек на перемещение данных и возможность эффективной реализации вычислительных алгоритмов на параллельной ВС, оснащенной ускорителми такого типа. Можно привести таблицу, аналогичную \ref{tab-interp-koef} с той поправкой, что эффективность многопоточного доступа к памяти на Intel Phi несколько ниже, поэтому значения интерполяционого коэффициента будут меньше для алгоритмов, использующих большой объем памяти.
