\chapter{Анализ масштабируемости, параллельной эффективности и ускорения параллельной ВС}
			%	ОБЩЕЕ ПРАВИЛО ДЛЯ ВТОРОЙ ГЛАВЫ: НЕ ПЛАНРИРОВАТЬ ЧТО_ТО НОВОЕ И ОЧЕНЬ ХОРОШЕЕ ЧЕГТО НЕТ, А ПОДКРЕЛЯТЬ И УИСИЛВАТЬ ТО, ЧТО РЕАЛЬНО ЕСТЬ.
			%	ЕСЛИ У НАС СЛАСБОСТЬ В ОБЛАСТИ АНАЛИЗА КОММ СТРУКТУРЫ - НУ И ЛАДНО, И ПУСТЬ ОНА БУДЕТ. ГОВОРИТЬ БОЛЬШЕ О МАСШТАБИРУЕМОСТИ. А КОРММ СТРУКТУРУ АНАЛИЗИРПОВАТЬ ЧЕРЕЗ ПЕРЕСЫЛКИ ЧАСТИЦ
			%	НАША ИДЕЯ не ПОДМЕНЯТЬ СОБОЙ ДИАГНОСТИКУ АРХИТЕКТУРЫ, а мерять И ОПРЕДЕЛЯТЬ ВСЕ ЧЕРЕЗ ИЗМЕРЕНИЯ ВРЕМЕН ПО PIC
				В четвертой главе предложена методика интегральной оценки тестируемой ВС с помощью измерения масштабируемости в расчетах по методу частиц в ячейках и определения возможности решения больших задач на ВВС на основе измерений возрастания потока данных в коммуникационной сети данной ВВС.
%				На основе данных о пересылке модельных частиц измерена производительность коммуникационной сети ВС, при этом важно отметить преимущества использованного метода измерений: пересылка данных имеет высокую степень нерегулярности, а также большой объем, что означает проведение тестирования на большой нагрузке, и возможность широкого применения полученных таким образом данных.
%				Также предложена и апробирована методика определения фактически соседних (с точки зрения MPI) узлов ВС.  

            Основной вопрос четвертой главы заключается в следующем: что можно сказать о вычислительной системе и о перспективах ее использования для решения различных задач на основании тестирования с помощью метода частиц в ячейках
             \cite{MohographyTarkov,VestnikNNSU,multigridAuto,AutoParSilan}. 
            
            В частности, в данной главе определяется новое понятие эффективного коммуникационного размера ВС, которое представляет собой максимальное количество процессоров, которое может быть в рамках данной ВС эффективно использовано для решения одной задачи.  
							
			
		
		\section{Формулы для анализа данных о масштабируемости}
		
			\begin{equation}
			\label{weak_eff}
			\eta^{weak}_N = \frac{T^1(1)}{T^N(N)}
			\end{equation}
			здесь $T^K(N)$ обозначает время счета задачи с характерной размерностью $K$ при использовании $N$ процессоров.
			это время состоит из двух основных частей:
			\begin{itemize}
				\item собственно времени счета $T^K_{P}(N)$;
				\item времени коммуникаций $T^K_C(N)$;
			\end{itemize}
			Таким образом,
			\begin{equation}
			\label{weak_eff_detailed}
			\eta^{weak}_N = \frac{T^1(1)}{T^N_{P}(N)+T^N_C(N)}
			\end{equation}
			при выполнении вычислений одновременно на однотипных процессорах
			можно считать, что $T^1(1) = T^N_{P}(N)$, таким образом формула
			\ref{weak_eff_detailed} приводится к виду:
			\begin{equation}
			\label{weak_eff_detailed-time}
			\eta^{weak}_N = \frac{1}{1+ \frac{T^N_{C}(N)}{T^1(1)}}.
			\end{equation}
			Это означает, что при известном времени расчета задачи на одном процессоре можно восстановить время коммуникаций по эффективности в слабом смысле:
			\begin{equation}
			\label{comm_time_from_efficiency}
			T^N_{C}(N) = T^1(1) \left(\frac{1}{\eta^{weak}_N} - 1\right)
			\end{equation}
			далее время коммуникаций может быть источником для вычисления производительности коммуникационной сети при известном объеме пересылок.
			
		\section{Структура временного шага метода частиц в ячейках}
		
		Для последующего анализа измеренного времени работы метода частиц в ячейках на параллельной ВС, и для прояснения зависимости этого времени от параметров расчета можно привести следующую схематическую формулу для длительности одного временного шага:
		
		\begin{equation}
		\label{PIC-timestep}
		T=T_{F} \left ( \frac{N_x N_y N_z }{P_E}\right )+ T_{F,S}\left (N_y N_z, P_E\right) + T_P\left(\frac{N_p}{P_E\times P_L}\right) +T_{P,S}\left (\frac{N_p*T_p}{P_E\times P_L}\right)
		\end{equation}
		
		здесь $T_{F}$ - время вычисления электромагнитного поля, $N_x, N_y, N_z$ - размер расчетной сетки соответственно, по координатам $X$, $Y$ и $Z$, $P_E$ - количество процессоров для эйлеровой декомпозиции, т.е. для разделения расчетной области на подобласти, $T_{F,S}$ - время, затрачиваемое на персылку граничных значений полей и токов между подобластями, которое зависит только от $N_y$ и $N_z$ в силу того что на данный момент используется одномерная декомпозиция,  $T_P$ - время расчета движения частиц, $P_L$ - количество процессов (или потоков) для лагранжевой декомпозиции, т.е. для дополнительного разделения частиц подобласти на группы, вычисляемые на отдельных ядрах.	
			
		
		
		
	
	    \section{Вычисление характеристик коммуникационного оборудования ВС на основе измеренной масштабируемости метода частиц в ячейках}
	    
	    
	    
	    В этом разделе показаны измеренные характеристики масштабируемости параллельной программы, реализующей метод частиц в ячейках на кластере НГУ. Для того, чтобы исключить вопрос о неточности измерений коомуникационного времени внутри программы, в этом разделе измерения проводились с помощью Intel Trace Analyzer \& Collector.
	    
\begin{figure}[h]
\begin{center}
\includegraphics[height=10cm,keepaspectratio]{images/scalingNSU-img17.png}
\caption{
Результат профилировки программы с помощью Intel Trace Analyzer \& Collector на кластере НГУ. Горизонтальные линии(трассы) означают отдельные параллельные процессы, тонкие линии между трассами – межпроцессорные коммуникации, номера процессов показаны слева (от 0 до 15, всего 32 процесса). Красный цвет (темно-серый для рисунка в оттенках серого) трассы означает пересылки, синий (светло-серый для рисунка в оттенках серого)-счет. Показан отдельный интервал расчета на сетке $800\times800\times20$, на 32 процессорах.
}
\label{scale8}
\end{center} 
\end{figure}
	   
	   
	    Результат показан на рисунках \ref{scale9} - \ref{scale10}. По рисункам можно \textbf{сделать следующий вывод}: измерение времени работы теста PIC-MANAS, проводимое его собственными средствами, находится в соответствии с данными внешних измерений, таким образом, проведен объективный контроль результатов тестирования.
	    
\begin{figure}[h]
	\begin{center}
		\includegraphics[height=10cm,keepaspectratio]{images/scalingNSU-img18.png}
		\caption{
		 Время выполнения коммуникационных операций (названия операций даны на рисунке, время показано красным (темно-серым для рисунка в оттенках серого) цветом в графе TSelf) в программе в сравнении с временем вычислений (Group Application, синий, или светло-серый цвет в графе TSelf). Расчет на сетке $1000\times1000\times4$ узла,10 временных шагов, 100 частиц в ячейке,  100 процессорных ядер. Результат профилировки программы с помощью Intel Trace Analyzer \& Collector на кластере НГУ.
		}
		\label{scale9}
	\end{center} 
\end{figure}	    
	   
	   
\begin{figure}[h]
	\begin{center}
		\includegraphics[height=10cm,keepaspectratio]{images/scalingNSU-img20.png}
		\caption{
			Время выполнения коммуникационных операций (названия операций даны на рисунке, время показано красным, или темно-серым цветом в графе TSelf) в программе в сравнении с временем вычислений (Group Application, синий, или светло-серый цвет в графе TSelf). Расчет на сетке $1000  \times 1000 \times 4$ узла, 200 частиц в ячейке, 400 процессорных ядер. Результат профилировки программы с помощью Intel Trace Analyzer\&Collector на кластере НГУ.
		}
		\label{scale9a}
	\end{center} 
\end{figure}	    
	    
	    
	   

\begin{figure}[h]
	\begin{center}
		\includegraphics[height=10cm,keepaspectratio]{images/scalingNSU-img21.png}
		\caption{
		Время выполнения коммуникационных операций (названия операций даны на рисунке, время показано красным (темно-серым для рисунка в оттенках серого) цветом в графе TSelf) в программе в сравнении с временем вычислений (Group Application, синий, или светло-серый цвет в графе TSelf). Расчет на сетке $1000 \times 1000 \times 4$ узла, 600 частиц в ячейке, 600 процессорных ядер. Результат профилировки программы с помощью Intel Trace Analyzer \&Collector на кластере НГУ
		}
		\label{scale10}
	\end{center} 
\end{figure}
	 

	    
	   
\begin{figure}[h]
	
	
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/scaleNSU_W_A_new.png}
		\caption{
		 Производительность коммуникационной сети для коллективных пересылок (величина $W_A$, формула \ref{Net_performance_collective}). Расчет на сетке $1000 \times 1000 \times 4$ узла. Измерение времени проведено с помощью Intel Trace Analyzer\& Collector на кластере НГУ.
		}
		\label{scale_W_A}
	\end{center} 
\end{figure}
     Здесь мы подходим к решению основного вопроса главы 4, и одного из основных для всей диссертационной работы в целом - а именно вопроса о том, что расчет по методу частиц может прояснить в отношении ВС на которой он выполнялся. Для этого определим новую характеристику ВС:
     
     \underline{\textbf{Определение}}: Эффективный коммуникационный размер ВС - максимальное количество процессоров, которое может быть в рамках данной ВС эффективно использовано для решения одной задачи.  
     
     <<Эффективно>> в данном случае означает без существенного падения пропускной способности коммуникационной сети на MPI-пересылках при увеличении количества используемых узлов. За существенное падение в данной работе принято падение более чем в $e$ раз, при том, что безусловно, это не единственно возможный выбор. 
     
     Особую важность представляет вопрос о коммуникационной связности ВС, т.е о том, до какой степени она способна функционировать как целое для решения одной большой задачи. Для решения этого вопроса предлагается перейти от масштабируемости как характеристики программы для решения задачи к анализу графика изменения производительности коммуникационной сети, полученного на основе данных о масштабируемости, т.е рисунке \ref{scale_W_A}. Далее необходимо тем или иным способом выделить на этом рисунке участок с большими значениями производительности коммуникационной сети.
     Один из возможных вариантов показан на рисунке \ref{scale_W_A_exp}, где показана аппроксимация этой зависимости гауссоидой. Далее дисперсия этой кривой (126) суммируется с начальной точкой (100), и полученное значение (226) и является эффективным коммуникационным размером данной ВС. Т.е. в данном случае эффективный коммуникационный размер - это количество процессоров, для которого производительность коммуникационной сети падает не более чем в $e$ раз по сравнению с максимальной.
     
     
\begin{figure}[h]
	
	
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/scaleNSU_exp_fit.png}
		\caption{
			Производительность коммуникационной сети для коллективных пересылок (величина $W_A$, формула \ref{Net_performance_collective}). 
		}
		\label{scale_W_A_exp}
	\end{center} 
\end{figure}

Далее вычислим эту же величину для двух наиболее характерных примеров зависимости эффективности в слабом смысле от числа процессорных ядер, представленных в диссертационной работе: для МВС-100К - сравнительно небольшая эффективность для большой разнородной ВС и для раздела ВВС <<Ломоносов>>, оснащенного GPU - очень высокая эффективность для однородной и компактной ВС. 

Напомним формулу для производительности коммуникационной сети ($W_A$, формула \ref{Net_performance_collective}):
$$
W_A = \frac{1}{P_{SUB}}\frac{N_X\times N_Y \times N_Z \times 24}{T_A}
$$
для расчета раздела ВВС <<Ломоносов>>, оснащенного GPU, $N_X = 102, N_Y = 6, N_Z = 6, P_{SUB}  = 1$

Значения $T_A$ приведены в таблице \ref{Lomonosov_GPU_W_A} и на рисунке \ref{scale_W_A_Lomonosov_GPU_exp_fit}. Эффективный коммуникационный размер в данном случае равен 239 (рисунок \ref{scale_W_A_Lomonosov_GPU_exp_fit}).

\begin{table}[ht]
\caption{Время выполнения операции MPI\_Allreduce для раздела ВВС <<Ломоносов>>, оснащенного GPU.}
\begin{tabular}{|c|c|c|}
	\hline
количество GPU & Время, сек. & Производительность  \\
               &             & коммуникационной сети, ГБ/сек. \\\hline
 10           & 0.029       & 0.0028\\
 50           & 0.032       & 0.00256\\
 100          & 0.033       & 0.00248\\
 200          & 0.035       & 0.0023\\
 400          & 0.039       & 0.0021\\
 500          & 0.041       & 0.0020\\	
\hline
\end{tabular}
\label{Lomonosov_GPU_W_A}
\end{table} 

\begin{figure}[h]
	
	
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/W_A_Lomonosov_on_weak_eff.png}
		\caption{
			Пропускная способность коммуникационной сети для коллективных пересылок (величина $W_S$, формула \ref{Net_performance_peer}). Расчет на сетке $102 \times 6 \times 6$ узла на <<Ломоносов>>, раздел с GPU. 
		}
		\label{scale_W_A_Lomonosov_GPU_exp_fit}
	\end{center} 
\end{figure}

\begin{figure}[h]
	
	
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/W_A_Lomonosov_Gauss.png}
		\caption{
				Производительность коммуникационной сети для коллективных пересылок (величина $W_A$, формула \ref{Net_performance_collective}). Расчет на сетке $100 \times 6 \times 6$ узла на <<Ломоносов>>, раздел с GPU. 
		}
		\label{scale_W_A_Lomonosov}
	\end{center} 
\end{figure}


Далее рассмотрим аналогичные показатели для расчетов на МВС-100К, с той разницей, что в них использовалась эйлерова декомпозиция, поэтому вычисляется не $W_A$, а $W_S$ (формула \ref{Net_performance_peer})
$$
W_S = \nu \frac{U_P\times  N_P \times P_{core}}{T_{S,PIC}}
$$
где:
\begin{itemize}
    \item $\nu$ - доля пересылаемых частиц (напомним, что обычно $\nu \approx 0.05$);
	\item $U_P$ - количество байт на одну модельную частицу, $U_P = 48$;
	\item $N_P$ - количество модельных частиц на одно процессорное ядро;  
	\item $P_{core}$ - количество ядер процессора, $P_{core} = 4$;

	\item $T_{S,PIC}$  - время пересылки частиц, сек.
\end{itemize}

Напомним формулу для вычисления времени коммуникаций на основе данных об эффективности в слабом смысле (формула \ref{comm_time_from_efficiency}):
$$
T^N_{C}(N) = T^1(1) \left(\frac{1}{\eta^{weak}_N} - 1\right)
$$
Эта формула будет использована для вычисления времени коммуникаций с той разницей, что в качестве базового будет использовано время не для одного ядра (такие расчеты не проводились в силу того, что нужный объем данных не помещается на одно ядро), а для 256 ядер,  $T^{256}(256) = 0.878$ сек., более того, формула изменится с учетом того, что в исходном варианте рассматривается однопроцессорный расчет, для которого время коммуникаций равно 0, и фактически, формула будет выглядеть следующим образом:
\begin{equation}
T^N_{C}(N) = T^256(256) \left(\frac{1}{\eta^{weak}_N} - 1\right) + T^{256}_C(256)
\label{comm_time_from_efficiency_256}
\end{equation}

Таким образом, необходимо по формуле \ref{comm_time_from_efficiency_256} вычислить время коммуникаций (фактически, время пересылки модельных частиц), и далее, подствив его в формулу  \ref{Net_performance_peer} с учетом размерности задачи, здесь $ T^{256}_C(256) = 0.011$ сек.
Количество частиц 
$$
N_P = N_Y\times N_Z\times N_{PIC}
$$
где $N_{PIC}$ - количество модельных частиц в ячейке ($N_{PIC} = 150$), $N_Y$ и $N_Z$ - количество узлов сетки соответственно, по Y и по Z, $N_Y = N_Z = 128$, количество узлов сетки по  X равно в данном случае количеству ядер, используемому в расчете, в итоге $N_P = 2.5\times 10^6$.

Эффективность $\eta^{weak}$, количество процессорных ядер, соответствующее каждому значению эффективности, и результирующее значение $W_S$ показаны в таблице \ref{MVS100K_W_S}.

\begin{table}[ht]
\caption{Эффективность в слабом смысле и производительность коммуникационной сети для МВС-100К.}
\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		количество  & Эффективность & Время пересылки & Производительность  \\
		ядер        & в слабом смысле & частиц, сек.    & коммуникационной сети, \\
		             &                 &                 &  ГБ/сек. \\\hline
		256           & 1.0             &     0.011       & 0.5\\
		512           & 0.79            &     0.25        & 0.02\\
		1024          & 0.74            &     0.32        & 0.02\\
		1536          & 0.66            &     0.46        & 0.01\\	
		2048          & 0.6             &     0.6         & 0.01 \\
		\hline
	\end{tabular}
	\label{MVS100K_W_S}
\end{center}
\end{table}

\begin{figure}[h]
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/W_S_MVS_100K_on_weak_eff.png}
		\caption{
			Производительность коммуникационной сети для парных пересылок (величина $W_S$, формула \ref{Net_performance_peer}). Расчет на сетке $N_X \times 128 \times 128$ узла на МВС-100К, здесь $N_X$ совпадает с количеством процессорных ядер. 
		}
		\label{scale_W_S_MVS_100K}
	\end{center} 
\end{figure}

Аналогично проведем аппроксимацию зависимости на рисунке \ref{scale_W_S_MVS_100K}, которая показана на рис. \ref{scale_W_S_MVS_100K_Gauss}. Эффективный коммуникационный размер в данном случае равен 425 (дисперсия гауссовой кривой равна 169, плюс начальная точка графика 256).

 
\begin{figure}[h]
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/W_S_MVS_100K_Gaussf.png}
		\caption{
			Производительность коммуникационной сети для парных пересылок (величина $W_S$, формула \ref{Net_performance_peer}) - синие точки, и ее аппроксимация гауссовой кривой. Расчет на сетке $N_X \times 128 \times 128$ узла на МВС-100К, здесь $N_X$ совпадает с количеством процессорных ядер.
		}
		\label{scale_W_S_MVS_100K_Gauss}
	\end{center} 
\end{figure}

      
\clearpage	    
	    
	 

		
		
		\section{Анализ масштабируемости как интегральной характеристики ВС.}
		
		В этом разделе приведены фактически измеренные на различных высокопроизводительных ВС графики масштабируемости и параллельной эффективности и на основе этих данных проведен анализ коммуникационной сети данных ВС, в частности, для МВС-100К, рис. \ref{eff2}. 
		
		\begin{figure}[h]
			\begin{center}
				\includegraphics[height=5cm,keepaspectratio]{images/eff_weak_JSCC.png}
				\caption{
					Эффективность распараллеливания в слабом смысле, для МВС-100К, МСЦ РАН.
				}
				\label{eff2}
			\end{center} 
		\end{figure}
		
		\subsection{Описание пересылаемых данных}
		
		Возможность проведения анализа коммуникационной сети с помощью расчетов по методу частиц в ячейках основана на известной информации о количестве пересылаемых данных и о виртуальной топологии, используемой в программе.
		
		Размер данных, перемещаемых между двумя соседними MPI-процессами равен $144 \times N_y \times N_z  $ байт. При этом в идеальном случае, когда соседние MPI-процессы находятся на соседних узлах, коммуникации происходят только между соседними узлами, и поток данных в системе в целом не возрастает с ростом количества используемых в расчете узлов.
		
		В частности, в расчете показанном на рис. \ref{eff2} использована эйлерова декомпозиция. Это означает, что используются только парные пересылки MPI, коллективные пересылки не используются, и поток данных через коммуникационную систему ВС в целом возрастать не должен. Если, тем не менее, он возрастает, что видно на рис. \ref{eff2} в виде снижения эффективности распараллеливания, то это может  означать (при отсутствии коллективных операций), что соседние с точки зрения MPI процессы находятся на физически удаленных друг от друга узлах параллельной ВС.
		
			\subsection{Вспомогательные величины для анализа масштабируемости}
		
		Введем обозначение $k_{||}$ - зависимость\textbf{ времени пересылок граничных условий} от \textbf{количества процессоров} в формуле \ref{PIC-timestep}, так что 
		\begin{equation}
		T_{F,S} = k_{||} (P_E) \frac{N_y N_z}{P_E}
		\end{equation} 
		и подставляя формулу \ref{PIC-timestep} d \ref{weak_eff}, рассматривая только лишь время расчета и пересылок электромагнитного поля, можно получить 
		\begin{equation}
		k_{||} (P_E) = \frac{1}{\eta^{weak}(P_E)} - 1
		\end{equation}	  
		Таким образом величина $k_{||} (P_E)$  - \textbf{степень нелинейности} коммуникационной структуры параллельной ВС. Фактически она представляет собой отклонение от линейной функции для зависимости времени пересылок от количества процессоров. Она показывает предел возрастания потока данных через коммуникационную структуру ВС при увеличении количества процессоров, используемых в расчете. Эта величина характеризует, в какой степени при передаче информации между соседними процессами в MPI используются узлы параллельной ВС, не являющиеся ближайшими соседями. В силу того, что на значение эффективности оказывает влияние не только свойства оборудования, но и особенности реализации MPI, возникает необходимость разделить эти факторы. Это достигается с помощью привязки процессоров к узлам. 
		
		В итоге, таким образом определенная  величина $k_{||} $ может быть использована как характеристика параллельной ВС, показывающая реально достижимую с помощью данной ВС эффективность и масштабируемость.
		
		Что касается масштабируемости как интегральной характеристики ВС, то хорошая масштабируемость на коллективных, очевидно нелинейных коммуникациях является важным показателем связности (сохранения пропускной способности сети при увеличении ее размера) ВС и пригодности ее к решению одной большой задачи.
		
		\subsection{Измерение продолжительности параллельных коммуникаций и анализ характеристик и топологии коммуникационного оборудования}
		Здесь описано решение задачи об определении соседства процессов по реальным узлам. Это исключительно важно для производительности реальных задач, чтобы виртуально близкие (т.е. по номеру MPI-процесса) процессы исполнялись бы на соседних узлах многопроцессорной ВС. Для этого проводится обмен сообщениями между узлами, выделенными 
		для исполнения программы по топологии полного графа, и проводится анализ времени прохождения сообщений, рис. \ref{poly_all2all}.  
		
		Следует отметить, что такого этапа, с обменом сообщениями между всеми процессами в рамках метода частиц нет, поэтому, такой анализ проводится предварительно, перед запуском основной части программы.

		
		\begin{figure}[htb]
			\begin{center}
				\includegraphics[height=7cm,keepaspectratio]{images/polytech_all_to_all.png}
			\end{center}
			\caption{Время пересылок для 40 MPI-процессов, расположенных на 10 узлах по 4 процесса, кластер «Политехник», СПбПУ. По осям X и Y отложены номера процессов, цветовая шкала показывает время пересылок в секундах.}
			\label{poly_all2all}
		\end{figure} 
		Узлы с минимальным временем считаются близкими, т.е. выясняется фактическая топология ВС. Это сопоставляется с известной информацией о размещении процессов по узлам.	Для повышения производительности приложения в дальнейшем целесообразно передвинуть соседние процессы на те узлы, где по факту меньше задержка по коммуникациям.
		
		%	\textbf{картинку из стьатьи все-со-всеми, (с Политеха)}. 
		%	1.измерение всех видов MPI-коммуникаций, сравнение одного с другим (Send, Isend, Bsend) - и увязатиь это с алгоритмом
		%	2. варьирование размера сообщений и пр. параметров
		
	%	\textbf{материал статьи НГУ ИТ  с более аакуратным анализом}
        \section{Оценки параллельной масштабируемости на основе измерений времени прохождения сообщений}
        
        Основные вопросы заключаются в следующем:
        \begin{itemize} 
        \item Что, если группа процессов, которые должны осуществлять коллективные пересылки, окажется размещенной на узлах ВВС, расположенных физически далеко друг от друга, так что время выполнения этих пересылок будет велико?
        \item Как можно избежать такой ситуации, как объединять в группы для коллективных пересылок близко расположенные процессы, при том что MPI (или система очередей) размещает процессы на узлах фактически случайным образом? 
        \item Как можно заранее оценить трудоемкость коммуникационных операций и выработать оптимальную схему размещения процессов? 
        \item На основании чего можно принимать решение о перемещении процессов с одного узла на другой, или о перенумерации процессов?
        \end{itemize} 
        В некоторых случаях эти вопросы решаются специально созданными внешними инструментами, но для рассматриваемой в данной работе программы целесообразным является иметь собственные средства для решения проблемы неудачного размещения на узлах.
        Для того, чтобы частично ответить на эти вопросы, были проведены вычислительные эксперименты на нескольких ВВС с измерением производительности различных коммуникационных операций, построены оценки трудоемкости этих операций и предложен метод выделения групп близко расположенных процессов. Создать правильное представление о реальной длительности коммуникаций между процессами в программе важно также для того, чтобы решить вопрос о целесообразности использования динамической балансировки и для выбора конкретного ее варианта , и о применении форм-факторов высокого порядка в методе частиц в ячейках. 
        
        \textbf{Краткое описание проведенных тестов}
        
        Задавались следующие основные параметры и числовые характеристики тестовых расчетов:
         \begin{itemize} 
        \item $N_X, N_Y, N_Z$  - размер сетки по каждому из измерений ($N_X$, $N_Y$ – от 100 до 500, $N_Z$ = 20)
        \item $P_{ALL}$  - общее количество процессорных ядер (до 1000)
        \item $P_{SUB}$  - число подобластей (до 20)
         \end{itemize} 
        Измерялись кроме физических величин, следующие времена (с помощью Intel Trace Analyzer \&Collector, https://software.intel.com/en-us/intel-trace-analyzer):
         \begin{itemize} 
        \item $T$ – длительность тестового расчета (50 временных шагов), сек.
       \item  $t$  - длительность временного шага, сек.
        \item $T_{MPI\_All}$ - длительность операции MPI\_Allreduce (суммирование токов по всей области), сек.
        \item $T_{MPI\_Send}$ - длительность операции MPI\_Sendrecv (обмен граничными значениями), сек.
        \end{itemize}
        
        
        
        Расчеты проводились на следующих ВВС:
        \begin{itemize}
        \item Кластер НГУ. Из различных имеющихся типов узлов использовались только узлы  HP BL2x220c G6, каждый из которых содержит две материнские платы, на каждой из которых: два 4-ядерных процессора Intel Xeon E5540 с тактовой частотой 2530 МГц и 16 ГБ ОЗУ.
        \item Кластер СПбПУ «Политехник». Использовались узлы с двумя 28-ядерными процессорами  Intel®Xeon® E5-2600 v3.
        \item Кластер «Ломоносов» в НИВЦ МГУ. Использовались узлы основного раздела, содержащие 2 4-ядерных процессора Intel Xeon  X5570. 
        \end{itemize}
        
        Физические параметры проводимых в данной работе расчетов соответствуют кинетическому режиму развития двухпотоковой неустойчивости, рассмотренному в [20].
        
        Для оценки времени работы коммуникационных процедур, в первую очередь MPI\_Allreduce и MPI\_Send/MPI\_Recv с целью выработки оптимальной схемы размещения процессов исходя из реально выделенных (системой очередей) вычислительных ресурсов, можно использовать следующие простые формулы:
        
$$
T = a \exp (b N)
$$
$$
T = a N^b
$$
$$
T = a +bN,
$$
        где $T$- время пересылки данных, $N$ – количество процессов (ядер), $a$ и $b$ – константы, зависящие от архитектуры ВВС, количества пересылаемых данных, реализации MPI и др. На рисунках \ref{lin_appr}-\ref{exp_appr} показаны результаты аппроксимации реальных данных о продолжительности коллективных пересылок, полученных в ходе вычислительных экспериментов, и на рисунках \ref{send_lin_appr}-\ref{send_exp_appr} - о продолжительности парных пересылок. На рисунках приведены аппроксимирующие формулы, и указано значение среднеквадратической ошибки, позволяющее определить наилучший тип аппроксимации. На рисунках \ref{lin_appr}-\ref{exp_appr} и \ref{send_lin_appr}-\ref{send_exp_appr} показаны данные, измеренные на всех кластерах, на которых проводились расчеты, т.е. особенности архитектуры здесь не учитываются.
        Каждая точка на рисунках \ref{lin_appr}-\ref{exp_appr} и \ref{send_lin_appr},\ref{send_power_appr},\ref{send_exp_appr} представляет собой отдельный расчет. Все расчеты проведены на разных архитектурах, количество ядер соответствует количеству MPI-процессов.
        
        \begin{figure}[htb]
        	\begin{center}
        		\includegraphics[height=7cm,keepaspectratio]{images/RomanenkoAASnytnikovAVChernykhIGadaptationtosupercomputerfinalEXTENDEDREFERENCES-img2.png}
        	\end{center}
        	\caption{Продолжительность операции MPI\_Allreduce. Линейная аппроксимация.}
        	\label{lin_appr}
        \end{figure}
        
        %Рис.2. Продолжительность операции MPI\_Allreduce. Линейная аппроксимация.
        
        \begin{figure}[htb]
        	\begin{center}
        		\includegraphics[height=7cm,keepaspectratio]{images/RomanenkoAASnytnikovAVChernykhIGadaptationtosupercomputerfinalEXTENDEDREFERENCES-img3.png}
        	\end{center}
        	\caption{Продолжительность операции MPI\_Allreduce. Степенная аппроксимация.}
        	\label{power_appr}
        \end{figure}
        
      %  Рис.3. Продолжительность операции MPI\_Allreduce. Степенная аппроксимация.
       
       
       \begin{figure}[htb]
       	\begin{center}
       		\includegraphics[height=7cm,keepaspectratio]{images/RomanenkoAASnytnikovAVChernykhIGadaptationtosupercomputerfinalEXTENDEDREFERENCES-img4.png}
       	\end{center}
       	\caption{Продолжительность операции MPI\_Allreduce. Экспоненциальная аппроксимация.}
       	\label{exp_appr}
       \end{figure} 
       % Рис.4. Продолжительность операции MPI\_Allreduce. Экспоненциальная аппроксимация.
        Из рисунков \ref{lin_appr},\ref{power_appr},\ref{exp_appr} видно, что наименьшее значение среднеквадратической ошибки достигнуто при степенной аппроксимации:
        $$
        T = 0.0056*N^{0.214},
        $$
        здесь $N$ - количество ядер.
        \clearpage
        
        Далее рассмотрим парные пересылки. Зависимость от числа ядер в данном случае возникает потому, что в таких пересылках задействованы все ядра, между которыми разделена область: MPI-процесс, работающий на каждом ядре пересылает данные обоим своим соседям. Фактически речь идет о длительности эйлерова этапа параллельного алгоритма.
        
         \begin{figure}[htb]
         	\begin{center}
         		\includegraphics[height=7cm,keepaspectratio]{images/RomanenkoAASnytnikovAVChernykhIGadaptationtosupercomputerfinalEXTENDEDREFERENCES-img5.png}
         	\end{center}
         	\caption{Продолжительность операции MPI\_Send. Линейная аппроксимация.}
         	\label{send_lin_appr}
         \end{figure}
         
         %Рис.2. Продолжительность операции MPI\_Allreduce. Линейная аппроксимация.
         
         \begin{figure}[htb]
         	\begin{center}
         		\includegraphics[height=7cm,keepaspectratio]{images/RomanenkoAASnytnikovAVChernykhIGadaptationtosupercomputerfinalEXTENDEDREFERENCES-img6.png}
         	\end{center}
         	\caption{ Продолжительность операции MPI\_Send. Степенная аппроксимация.}
         	\label{send_power_appr}
         \end{figure}
         
         %  Рис.3. Продолжительность операции MPI\_Allreduce. Степенная аппроксимация.
         
         
         \begin{figure}[htb]
         	\begin{center}
         		\includegraphics[height=7cm,keepaspectratio]{images/RomanenkoAASnytnikovAVChernykhIGadaptationtosupercomputerfinalEXTENDEDREFERENCES-img7.png}
         	\end{center}
         	\caption{Продолжительность операции MPI\_Send. Экспоненциальная аппроксимация.}
         	\label{send_exp_appr}
         \end{figure} 
         
        В этом случае наименьшая ошибка оказалась достигнута при экспоненциальной аппроксимации:
        
        $$
        T = 0.0077*exp(-0.00562*N),
        $$
         здесь $N$ - количество ядер.
                 
        Полученные формулы не является зависимостями, работающими всегда, на любых ВВС, любых реализациях MPI и т.д. Их назначение в том, чтобы в ходе реального крупномасштабного расчета, без повторных запусков, не прерывая счет, ответить на вопрос, что будет, если увеличить количество процессоров (ядер), вовлеченных в коллективные взаимодействия. Т.е. в данном случае предполагается динамическое дозапускание MPI процессов, допустимое стандартом MPI2.
        Например, программа, использующая 40 MPI-процессов  получила для счета 40 ядер. Эти ядра можно по-разному распределить между эйлерой и лагранжевой декомпозицией области: можно поделить область на 10 частей, и затем распределить все частицы каждой подобласти между  4 MPI-процессами, а можно наоборот. Даже для 40 процессов есть несколько вариантов, в то время как речь идет о расчетах на нескольких тысячах ядер, где невозможно будет просто перебрать все варианты и выбрать оптимальный.
        Для того, чтобы сделать правильный выбор, предлагается после выделения узлов для счета провести несколько тестовых запусков коллективных операций, построить для данного конкретного расчета аппроксимацию, подобную полученной выше, и на ее основе принимать решение.
        
        
       
        
        \section{Опредедение зависимости масштабируемости от наличия и типа ускорителей вычислений}
        
        \textbf{Масштабируемость для кластера на основе Intel Xeon Phi.} 
        
       Также было проведено тестирование масштабируемости программы на кластере RSC Petastream в МСЦ РАН, результат показан на рис. \ref{phi100}. Видно, что в целом программа хорошо масштабируется, локальный максимум, видимый на графике при 5 ускорителях, может объясняться тем, что система на тот момент работала в тестовом режиме.
        
        \begin{figure}[htb]
        	\begin{center}
        		\includegraphics[height=7cm,keepaspectratio]{images/petastream_phi100.jpg}
        	\end{center}
        	\caption{Эффективность распараллеливания в слабом смысле с использованием Intel Xeon Phi. Расчет проведен на суперкомпьютере RSC Petastream, МСЦ РАН.}
        	\label{phi100}
        \end{figure}
        
        Также было выполнено распалаллеливание на мелкозернистом уровне: по отдельным частицам в рамках ячейки: отдельные вычислительные потоки назначаются для счета траекторий отдельных частиц. Это было выполнено с помощью технологии CUDA для графических ускорителей (GPU) и с помощью технологии OpenMP для ускорителей Intel Xeon Phi. Программа MANAS может быть скомпилирована как для одного типа ускорителей (GPU). Так и для другого (Intel Xeon Phi). Это достигается с помощью процедурных переменных и директив условной компиляции. Программа протестирована на кластере НКС-30Т (ИВМиМГ СО РАН), графические ускорители Nvidia Tesla M2090 (до 10) и Nvidia Kepler K40 и на «Ломоносов»  (до 500 Nvidia Tesla C2070). Средняя продолжительность одного временного шага 0.01 секунды для  Nvidia Kepler K40 и 0.13 секунды для  Intel Xeon Phi (сетка 250х250, 100 частиц).
        
        \begin{figure}[htb]
        	\begin{center}
        		\includegraphics[height=7cm,keepaspectratio]{images/lomonosov_gpu500.jpg}
        	\end{center}
        	\caption{Эффективность распараллеливания в слабом смысле. Расчет проведен на суперкомпьютере «Ломоносов», НИВЦ МГУ.}
        	\label{gpu500}
        \end{figure}
        
        Из сравнения рисунков \ref{phi100} и \ref{gpu500} видно, что наличие ускорителей вычислений не дает каких-то особенностей в работе коммуникационной системы ВС, и таким образом, коммуникационная система может анализироваться независимо от типа узлов ВС.
          
        
        
        
        
        
        
 
        
       
        
        
		\section{Измерение производительности коммуникационной сети на основе данных о пересылке модельных частиц}
		Приведены результаты измерения скорости пересылок модельных частиц в методе частиц в ячейках и проведенный на основе этого сравнительный анализ скорости работы коммуникационной сети ВС.
		
		На нескольких параллельных ВС были измерены времена, затраченные на пересылку модельных частиц между соседними узлами. Из статитики расчетов по методу частиц в ячейках известно, что пересылается, как правило, не более 5\% модельных частиц. В рассматриваемых расчетах размер сетки $512\times 64 \times 64$ узла  при 150 модельных частицах в ячейке, т.е.  на каждом временном шаге в среднем пересылается 15.7 млн. модельных частиц, что при размере одной частицы в 48 байт означает 720 Мб на каждый временной шаг, что при известном времени пересылки позволяет вычислить скорость. Таким образом была экспериментально измерена величина $T_{P,S}$, входящая в формулу \ref{PIC-timestep}   
			
			
			
			Измеренная таким образом скорость пересылки данных показывает фактический предел этой величины, реально достижимый для вычислительного приложения с использованием имеющегося оборудования и коммуникационного программного обеспечения. Это подтверждается следующими соображениями:
			\begin{itemize}
				\item Объем пересылаемых данных мал: в среднем 6 Мб на процесс
				\item Соседние MPI-процессы, как правило, расположены на близких узлах.
			\end{itemize}  	 
			
		%	\subsubsection{Экстраполяция результатов тестирования ускорения и эффективности распараллеливания}
			%%	формулы из статьи НГУ-ИТ'17, адаптированные и улучшенные, сравнение со Степаненко
			%%	выводы по большой системе на тех же принципах, совпадение с реальностью
			%%	
			%	В \textit{четвертом разделе} описана оригинальная методика экстраполяции результатов на системы большой размерности и ее сравнение с аналогиными работами.Проведено большое количество физических расчетов с использованием программы в трехмерной расчетной области, использующей двухступенчатую эйлерово-лагранжеву декомпозицию расчетной области.На основании тестовых расчетов на небольшом количестве процессорных ядер измереятся время коллективных и парных коммуникаций MPI и строится аппроксимацию времени пересылок для произвольного количества процессов.
			
			%	Основным вопросом является соответствие реально измеряемого времени выполнения коммуникационных операций для определенного количества процессоров, 
			%	 По результатам вырабатываются рекомендации по выбору оптимального сочетания эйлеровой и лагранжевой декомпозиции.
			%	\textbf{формулы}
			
			%	\subsection{Глава 3: Анализ производительности системы памяти}
			%	\subsubsection{Кэш-память}    
			%	Статьи PACO (кэш, списки и пр.), BOE (данны исполения на Sun Sparc Opteron)
			%	\subsubsection{Оперативная память}
			%	 PAVT10, Абрау7 PACO
			
\section{Сравнение с известными тестами производительности}

\subsection{Краткое описание теста IMB}

Тесты Intel® MPI Benchmarks (IMB) \cite{IMB} выполняют набор измерений производительности MPI для коммуникаций типа <<точка-точка>> и коллективных коммуникаций для определенного диапазона размеров сообщений. Тесты Intel® MPI разработаны с использованием стандартного MPI стандарта ANSI C plus. Он распространяется как проект с открытым исходным кодом, позволяющий использовать эталонные тесты в различных кластерных архитектурах и реализациях MPI.

Выходные данные теста описывают производительность кластерной системы, включая 
\begin{itemize}
	\item производительность узлов, 
	\item латентность сети  
	\item пропускную способность сети
	\item эффективность используемой реализации MPI
\end{itemize}

Пакет Intel® MPI Benchmarks состоит из нескольких компонентов, из которых в рамках данной работы для сравнения с разработанным тестом использовалась только одна - IMB-MPI1 - измерение показателей для функций MPI-1. Компоненты, характеризующие эффективность функций MPI стандарта 2.0 и выше, а также параллельного ввода-вывода MPI-IO, не рассматривались.

\subsection{Сравнение результатов теста IMB и PIC-MANAS} 

В этом разделе рассмотрено сравнение результатов тестирования с помощью теста IMB для некоторых из рассмотренных в главе 4 вычислительных систем. Пропускная способность коммнуникационной сети, измеренная с помощью теста PIC-MANAS для кластера НГУ была показана на рисунке \ref{scale_W_A}, как видно, она намного меньше, чем величины, приведенные для кластера НГУ на рисунке \ref{IMB}.
Причины такого различия могут заключаться в том, что тест Intel MPI Bebchmarks  возможно, лучше учитывает архитектуру ВС, и более оптимальным образом выполняет пересылки. Тем не менее, есть причина считать именно результаты теста PIC-MANAS более правильными. Эта причина заключаются в следующем: тест PIC-MANAS, построенный на основе реального расчетного приложения, дает пропускную способность более близкую к той, с которой столкнется автор другого физико-математического расчетного приложения.  


Видно, что в целом результаты тестирования совпадают с полученными по тесту PIC-MANAS и показанными в четвертой главе диссертации.


\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/imb.png}
	\end{center}
	\caption{Результаты тестирования с помощью теста Intel MPI Benchmarks, в мегабайтах в секунду.}
	\label{IMB}
\end{figure}

			