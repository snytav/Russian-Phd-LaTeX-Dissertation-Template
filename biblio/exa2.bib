@article{Keyes201170,
title = {Exaflop/s: The why and the how },
journal = {Comptes Rendus Mécanique },
volume = {339},
number = {2–3},
pages = {70 - 77},
year = {2011},
note = {High Performance ComputingLe Calcul Intensif },
issn = {1631-0721},
doi = {http://dx.doi.org/10.1016/j.crme.2010.11.002},
url = {http://www.sciencedirect.com/science/article/pii/S1631072110002032},
author = {David E. Keyes},
}

@article{Murugan2013,
title = {On the interconnect energy efficiency of high end computing systems },
journal = {Sustainable Computing: Informatics and Systems },
volume = {3},
number = {2},
pages = {49 - 57},
year = {2013},
note = {},
issn = {2210-5379},
doi = {http://dx.doi.org/10.1016/j.suscom.2012.03.002},
url = {http://www.sciencedirect.com/science/article/pii/S2210537912000194},
author = {Muthukumar Murugan and David Hung Chang Du and Krishna Kant},
keywords = {Green HPC},
keywords = {Interconnect},
keywords = {Energy efficiency},
keywords = {HPC scheduling},
keywords = {Extensible bin packing },
abstract = {High performance computing systems are moving towards the exaflops era. The tremendous increase in computational speed is accompanied by enormous power consumption in these systems. It is necessary to harvest any potential opportunities to save power in these high end computing systems. The goal of this paper is to explore possibilities of power savings in the interconnects between the nodes. By careful scheduling of jobs in a 3D torus-connected cluster of nodes, we show that significant amounts of power can be saved by switching certain portions of the network elements to low power modes. We also present an estimation method that more accurately estimates the actual runtime of jobs from the user provided runtimes and enhances the performance of the scheduling scheme. We validate our results via detailed \{MATLAB\} simulations. {
}

@article{Attig2011,
title = {Trends in supercomputing: The European path to exascale },
journal = {Computer Physics Communications },
volume = {182},
number = {9},
pages = {2041 - 2046},
year = {2011},
note = {Computer Physics Communications Special Edition for Conference on Computational Physics Trondheim, Norway, June 23-26, 2010 },
issn = {0010-4655},
doi = {http://dx.doi.org/10.1016/j.cpc.2010.11.011},
url = {http://www.sciencedirect.com/science/article/pii/S0010465510004571},
author = {N. Attig and P. Gibbon and Th. Lippert},
keywords = {Supercomputing},
keywords = {Exascale},
keywords = {Simulation Laboratory },
abstract = {Recent developments in European supercomputing are reviewed covering both the latest hardware trends and the increasing difficulties faced by scientists in utilising these machines to perform large-scale numerical simulations. These challenges are reflected in the large number of international initiatives which have come into being over the last few years, founded in anticipation of exascale hardware which is foreseen within the next decade. The role of a key institution in supercomputing within these programmes is described using the example of the Jülich Supercomputing Centre (JSC), and progress in setting up its own community-oriented support units for scientific computing – Simulation Laboratories – is reported on. Finally, an assessment is made of some common grand challenges and their suitability for scaling to exaflop-scale computation. {
}

@article{Attig2009,
title = {Computational physics with PetaFlops computers },
journal = {Computer Physics Communications },
volume = {180},
number = {4},
pages = {555 - 558},
year = {2009},
note = {Special issue based on the Conference on Computational Physics 2008CCP 2008 },
issn = {0010-4655},
doi = {http://dx.doi.org/10.1016/j.cpc.2008.12.032},
url = {http://www.sciencedirect.com/science/article/pii/S0010465508004499},
author = {Norbert Attig},
keywords = {Supercomputing},
keywords = {IBM Blue Gene},
keywords = {CPMD },
abstract = {Driven by technology, Scientific Computing is rapidly entering the PetaFlops era. The Jülich Supercomputing Centre (JSC), one of three German national supercomputing centres, is focusing on the \{IBM\} Blue Gene architecture to provide computer resources of this class to its users, the majority of whom are computational physicists. Details of the system will be discussed and applications will be described which significantly benefit from this new architecture. {
}

@article{Yavits2014,
title = {The effect of communication and synchronization on Amdahl’s law in multicore systems },
journal = {Parallel Computing },
volume = {40},
number = {1},
pages = {1 - 16},
year = {2014},
note = {},
issn = {0167-8191},
doi = {http://dx.doi.org/10.1016/j.parco.2013.11.001},
url = {http://www.sciencedirect.com/science/article/pii/S0167819113001324},
author = {L. Yavits and A. Morad and R. Ginosar},
keywords = {Multicore},
keywords = {Analytical Performance Models},
keywords = {Amdahl’s law },
abstract = {Abstract This work analyses the effects of sequential-to-parallel synchronization and inter-core communication on multicore performance, speedup and scaling from Amdahl’s law perspective. Analytical modeling supported by simulation leads to a modification of Amdahl’s law, reflecting lower than originally predicted speedup, due to these effects. In applications with high degree of data sharing, leading to intense inter-core connectivity requirements, the workload should be executed on a smaller number of larger cores. Applications requiring intense sequential-to-parallel synchronization, even highly parallelizable ones, may better be executed by the sequential core. To improve the scalability and performance speedup of a multicore, it is as important to address the synchronization and connectivity intensities of parallel algorithms as their parallelization factor. {
}

@article{Peterson1989,
title = {Computational challenges in aerospace },
journal = {Future Generation Computer Systems },
volume = {5},
number = {2–3},
pages = {243 - 258},
year = {1989},
note = {Grand Challenges to Computational Science },
issn = {0167-739X},
doi = {http://dx.doi.org/10.1016/0167-739X(89)90044-7},
url = {http://www.sciencedirect.com/science/article/pii/0167739X89900447},
author = {Victor L. Peterson},
abstract = {Computer speed and memory requirements needed for meeting various computational challenges in human vision modeling, chemistry, turbulence physics research and aerodynamics are discussed and compared with the capabilities of various existing computers and those projected to be available before the mid 1990s. Example results for problems illustrative of those currently being solved in each of the disciplines are also presented. Meeting some of the challenges using currently available solution algorithms is shown to require computer speeds in excess of exaFLOPs (1018 FLOPs) and memories in excess of petawords (1015 words), if problems are to be solved in periods of time currently believed to be acceptable. Even without these levels of computer power, it is shown how work can proceed towards meeting the ultimate challenges by treating stepping-stone problems with complexity increasing to match the computational power available at any point in time. Finally, it is speculated that improvements in algorithms ultimately will reduce these requirements to levels that can be met with computers projected to be available beyond the year 2000. {
}

@article{Schreiber2014,
title = {A few bad ideas on the way to the triumph of parallel computing },
journal = {Journal of Parallel and Distributed Computing },
volume = {74},
number = {7},
pages = {2544 - 2547},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing },
issn = {0743-7315},
doi = {http://dx.doi.org/10.1016/j.jpdc.2013.10.006},
url = {http://www.sciencedirect.com/science/article/pii/S0743731513002177},
author = {Robert Schreiber},
keywords = {Parallelism},
keywords = {Amdahl},
keywords = {Automatic parallelization},
keywords = {Accelerators},
keywords = {Exascale },
abstract = {Abstract Parallelism has become mainstream, in the multicore chip, the GPU, and the internet datacenter running MapReduce. In my field, large-scale scientific computing, parallelism now reigns triumphant. It was no simple, direct route that led to this triumph. Along the way, we were confused by ideas that, in retrospect, turned out to be distractions and errors. The thinking behind them was reasonable, but wrong. One can learn from a dissection of mistakes, so I will retell part of the story here. {
}

@article{Mittal20161065,
title = {Computational modeling of cardiac hemodynamics: Current status and future outlook },
journal = {Journal of Computational Physics },
volume = {305},
number = {},
pages = {1065 - 1082},
year = {2016},
note = {},
issn = {0021-9991},
doi = {http://dx.doi.org/10.1016/j.jcp.2015.11.022},
url = {http://www.sciencedirect.com/science/article/pii/S0021999115007627},
author = {Rajat Mittal and Jung Hee Seo and Vijay Vedula and Young J. Choi and Hang Liu and H. Howie Huang and Saurabh Jain and Laurent Younes and Theodore Abraham and Richard T. George},
keywords = {Hemodynamics},
keywords = {Computational fluid dynamics},
keywords = {Cardiac physics},
keywords = {Blood flow},
keywords = {Cardiovascular disease},
keywords = {Heart disease},
keywords = {Cardiac surgery},
keywords = {Left ventricular thrombosis},
keywords = {Heart murmurs},
keywords = {Cardiac auscultation},
keywords = {Image segmentation},
keywords = {Immersed boundary methods },
abstract = {Abstract The proliferation of four-dimensional imaging technologies, increasing computational speeds, improved simulation algorithms, and the widespread availability of powerful computing platforms is enabling simulations of cardiac hemodynamics with unprecedented speed and fidelity. Since cardiovascular disease is intimately linked to cardiovascular hemodynamics, accurate assessment of the patient's hemodynamic state is critical for the diagnosis and treatment of heart disease. Unfortunately, while a variety of invasive and non-invasive approaches for measuring cardiac hemodynamics are in widespread use, they still only provide an incomplete picture of the hemodynamic state of a patient. In this context, computational modeling of cardiac hemodynamics presents as a powerful non-invasive modality that can fill this information gap, and significantly impact the diagnosis as well as the treatment of cardiac disease. This article reviews the current status of this field as well as the emerging trends and challenges in cardiovascular health, computing, modeling and simulation and that are expected to play a key role in its future development. Some recent advances in modeling and simulations of cardiac flow are described by using examples from our own work as well as the research of other groups. {
}

@article{Lim2015128,
title = {Technological forecasting of supercomputer development: The March to Exascale computing },
journal = {Omega },
volume = {51},
number = {},
pages = {128 - 135},
year = {2015},
note = {},
issn = {0305-0483},
doi = {http://dx.doi.org/10.1016/j.omega.2014.09.009},
url = {http://www.sciencedirect.com/science/article/pii/S0305048314001200},
author = {Dong-Joon Lim and Timothy R. Anderson and Tom Shott},
keywords = {Data envelopment analysis},
keywords = {Technological forecasting},
keywords = {State of the art},
keywords = {Rate of change},
keywords = {Supercomputer },
abstract = {Abstract Advances in supercomputers have come at a steady pace over the past 20 years. The next milestone is to build an Exascale computer however this requires not only speed improvement but also significant enhancements for energy efficiency and massive parallelism. This paper examines technological progress of supercomputer development to identify the innovative potential of three leading technology paths toward Exascale development: hybrid system, multicore system and manycore system. Performance measurement and rate of change calculation were made by technology forecasting using data envelopment analysis (TFDEA.) The results indicate that the current level of technology and rate of progress can achieve Exascale performance between early 2021 and late 2022 as either hybrid systems or manycore systems. {
}

@incollection{Onishi2014,
title = {Optimized preprocessing of tens of billions of grids in a full-vehicle aerodynamic simulation on the K-computer },
editor = {Park, Holywell },
booktitle = {The International Vehicle Aerodynamics Conference },
publisher = {Woodhead Publishing},
edition = {},
address = {Oxford},
year = {2014},
pages = {149 - 158},
isbn = {978-0-08-100199-8},
doi = {http://dx.doi.org/10.1533/9780081002452.4.149},
url = {http://www.sciencedirect.com/science/article/pii/B9780081001998500137},
author = {K. Onishi and M. Tsubokura},
abstract = {\{ABSTRACT\} A vehicle aerodynamics simulation was conducted using 2.3 billion elements of an unstructured grid and 19 billion elements of a Cartesian grid with dirty computeraided-design data on the supercomputer K-computer. This methodology allows the user to avoid a large amount of manual work in preparing computational grids was developed using a mesh refinement technique and immersed boundary method. This methodology was indispensable in conducting fine-resolution analysis in a massively parallel environment. The calculation results show that the method was successfully adopted for full-vehicle aerodynamics and that the accuracy of drag prediction can be improved using fine grid resolution. {
}

@article{Xu2015200,
title = {Engineering molecular dynamics simulation in chemical engineering },
journal = {Chemical Engineering Science },
volume = {121},
number = {},
pages = {200 - 216},
year = {2015},
note = {2013 Danckwerts Special Issue on Molecular Modelling in Chemical Engineering },
issn = {0009-2509},
doi = {http://dx.doi.org/10.1016/j.ces.2014.09.051},
url = {http://www.sciencedirect.com/science/article/pii/S0009250914005557},
author = {Ji Xu and Xiaoxia Li and Chaofeng Hou and Limin Wang and Guangzheng Zhou and Wei Ge and Jinghai Li},
keywords = {Engineering \{MD\} simulation},
keywords = {Chemical processes},
keywords = {Meso-scale},
keywords = {Multiscale},
keywords = {EMMS paradigm},
keywords = {Particle methods },
abstract = {Abstract Chemical engineering systems usually involve multiple spatio-temporal scales, grouped into different levels, from the molecular scale of reactants to the industrial scale of reactors. Molecular dynamics (MD) simulation is one of the most fundamental methods for the study of such systems, but it is too costly and hence formidable for simulating large-scale behavior directly. However, there are two great potentials in extending this method. First, the logic and algorithms of traditional \{MD\} simulations can be generalized from the material level to higher levels since the elements of each level are all discrete in nature, and can be well defined, allowing an MD-style simulation based on different elements. Second, \{MD\} simulations can be accelerated by realizing the structural consistency among the problem, model, software and hardware (the so-called \{EMMS\} paradigm). These two potentials give possibilities to engineer the method of \{MD\} simulation to deal with the whole spectrum of chemical engineering phenomena. In this review, we summarize our discrete simulation studies to explore such potentials, from the establishment of a general software and hardware framework, to the typical applications at different levels, including the reactions in coal pyrolysis, the dynamics in virion, the atomic behavior in silicon at millimeter scale, and finally continuum flow. The possibility of engineering \{MD\} simulation into a virtual experiment platform is discussed finally. {
}

@article{Rajovic2014322,
title = {Tibidabo1: Making the case for an ARM-based \{HPC\} system },
journal = {Future Generation Computer Systems },
volume = {36},
number = {},
pages = {322 - 334},
year = {2014},
note = {Special Section: Intelligent Big Data ProcessingSpecial Section: Behavior Data Security Issues in Network Information PropagationSpecial Section: Energy-efficiency in Large Distributed Computing ArchitecturesSpecial Section: eScience Infrastructure and Applications },
issn = {0167-739X},
doi = {http://dx.doi.org/10.1016/j.future.2013.07.013},
url = {http://www.sciencedirect.com/science/article/pii/S0167739X13001581},
author = {Nikola Rajovic and Alejandro Rico and Nikola Puzovic and Chris Adeniyi-Jones and Alex Ramirez},
keywords = {High-performance computing},
keywords = {Embedded processors},
keywords = {Mobile processors},
keywords = {Low power},
keywords = {Cortex-A9},
keywords = {Cortex-A15},
keywords = {Energy efficiency },
abstract = {Abstract It is widely accepted that future \{HPC\} systems will be limited by their power consumption. Current \{HPC\} systems are built from commodity server processors, designed over years to achieve maximum performance, with energy efficiency being an after-thought. In this paper we advocate a different approach: building \{HPC\} systems from low-power embedded and mobile technology parts, over time designed for maximum energy efficiency, which now show promise for competitive performance. We introduce the architecture of Tibidabo, the first large-scale \{HPC\} cluster built from \{ARM\} multicore chips, and a detailed performance and energy efficiency evaluation. We present the lessons learned for the design and improvement in energy efficiency of future \{HPC\} systems based on such low-power cores. Based on our experience with the prototype, we perform simulations to show that a theoretical cluster of 16-core \{ARM\} Cortex-A15 chips would increase the energy efficiency of our cluster by 8.7×, reaching an energy efficiency of 1046 MFLOPS/W. {
}

@article{Straatsma2013,
title = {On eliminating synchronous communication in molecular simulations to improve scalability },
journal = {Computer Physics Communications },
volume = {184},
number = {12},
pages = {2634 - 2640},
year = {2013},
note = {},
issn = {0010-4655},
doi = {http://dx.doi.org/10.1016/j.cpc.2013.01.009},
url = {http://www.sciencedirect.com/science/article/pii/S001046551300026X},
author = {T.P. Straatsma and Daniel G. Chavarría-Miranda},
keywords = {One-sided communication},
keywords = {Global arrays},
keywords = {Molecular dynamics },
abstract = {Molecular dynamics simulation, as a complementary tool to experimentation, has become an important methodology for the understanding and design of molecular systems as it provides access to properties that are difficult, impossible or prohibitively expensive to obtain experimentally. Many of the available software packages have been parallelized to take advantage of modern massively concurrent processing resources. The challenge in achieving parallel efficiency is commonly attributed to the fact that molecular dynamics algorithms are communication intensive. This paper illustrates how an appropriately chosen data distribution and asynchronous one-sided communication approach can be used to effectively deal with the data movement within the Global Arrays/ARMCI programming model framework. A new put_notify capability is presented here, allowing the implementation of the molecular dynamics algorithm without any explicit global or local synchronization or global data reduction operations. In addition, this push-data model is shown to very effectively allow hiding data communication behind computation. Rather than data movement or explicit global reductions, the implicit synchronization of the algorithm becomes the primary challenge for scalability. Without any explicit synchronous operations, the scalability of molecular simulations is shown to depend only on the ability to evenly balance computational load. {
}

@article{Decyk2011,
title = {Adaptable Particle-in-Cell algorithms for graphical processing units },
journal = {Computer Physics Communications },
volume = {182},
number = {3},
pages = {641 - 648},
year = {2011},
note = {},
issn = {0010-4655},
doi = {http://dx.doi.org/10.1016/j.cpc.2010.11.009},
url = {http://www.sciencedirect.com/science/article/pii/S0010465510004558},
author = {Viktor K. Decyk and Tajendra V. Singh},
keywords = {Particle-in-Cell},
keywords = {GPU},
keywords = {Parallel algorithms },
abstract = {We developed new parameterized Particle-in-Cell algorithms and data structures for emerging multi-core and many-core architectures. Four parameters allow tuning of this \{PIC\} code to different hardware configurations. Particles are kept ordered at each time step. The first application of these algorithms is to \{NVIDIA\} graphical processing units, where speedups of about 15–25 compared to an Intel Nehalem processor were obtained for a simple 2D electrostatic code. Electromagnetic codes are expected to get higher speedups due to their greater computational intensity. {
}

@article{Decyk2014,
title = {Particle-in-Cell algorithms for emerging computer architectures },
journal = {Computer Physics Communications },
volume = {185},
number = {3},
pages = {708 - 719},
year = {2014},
note = {},
issn = {0010-4655},
doi = {http://dx.doi.org/10.1016/j.cpc.2013.10.013},
url = {http://www.sciencedirect.com/science/article/pii/S001046551300341X},
author = {Viktor K. Decyk and Tajendra V. Singh},
keywords = {Parallel algorithms},
keywords = {Particle-in-Cell},
keywords = {GPU},
keywords = {CUDA},
keywords = {Plasma simulation },
abstract = {Abstract We have designed Particle-in-Cell algorithms for emerging architectures. These algorithms share a common approach, using fine-grained tiles, but different implementations depending on the architecture. On the GPU, there were two different implementations, one with atomic operations and one with no data collisions, using \{CUDA\} C and Fortran. Speedups up to about 50 compared to a single core of the Intel i7 processor have been achieved. There was also an implementation for traditional multi-core processors using OpenMP which achieved high parallel efficiency. We believe that this approach should work for other emerging designs such as Intel Phi coprocessor from the Intel \{MIC\} architecture. {
}

@article{Stantchev20081339,
title = {Fast parallel Particle-To-Grid interpolation for plasma \{PIC\} simulations on the \{GPU\} },
journal = {Journal of Parallel and Distributed Computing },
volume = {68},
number = {10},
pages = {1339 - 1349},
year = {2008},
note = {General-Purpose Processing using Graphics Processing Units },
issn = {0743-7315},
doi = {http://dx.doi.org/10.1016/j.jpdc.2008.05.009},
url = {http://www.sciencedirect.com/science/article/pii/S0743731508000944},
author = {George Stantchev and William Dorland and Nail Gumerov},
keywords = {\{GPU\} computing},
keywords = {Scientific computing},
keywords = {Parallel algorithms},
keywords = {Numerical simulations},
keywords = {Particle-In-cell methods},
keywords = {Plasma physics },
abstract = {Particle-In-Cell (PIC) methods have been widely used for plasma physics simulations in the past three decades. To ensure an acceptable level of statistical accuracy relatively large numbers of particles are needed. State-of-the-art Graphics Processing Units (GPUs), with their high memory bandwidth, hundreds of \{SPMD\} processors, and half-a-teraflop performance potential, offer a viable alternative to distributed memory parallel computers for running medium-scale \{PIC\} plasma simulations on inexpensive commodity hardware. In this paper, we present an overview of a typical plasma \{PIC\} code and discuss its \{GPU\} implementation. In particular we focus on fast algorithms for the performance bottleneck operation of Particle-To-Grid interpolation. {
}

@article{Kong2011,
title = {Particle-in-cell simulations with charge-conserving current deposition on graphic processing units },
journal = {Journal of Computational Physics },
volume = {230},
number = {4},
pages = {1676 - 1685},
year = {2011},
note = {},
issn = {0021-9991},
doi = {http://dx.doi.org/10.1016/j.jcp.2010.11.032},
url = {http://www.sciencedirect.com/science/article/pii/S0021999110006479},
author = {Xianglong Kong and Michael C. Huang and Chuang Ren and Viktor K. Decyk},
keywords = {Graphics processing unit (GPU)},
keywords = {Computer unified device architecture (CUDA)},
keywords = {Particle-in-cell (PIC) plasma simulation },
abstract = {We present an implementation of a 2D fully relativistic, electromagnetic particle-in-cell code, with charge-conserving current deposition, on parallel graphics processors (GPU) with CUDA. The \{GPU\} implementation achieved a one particle-step process time of 2.52 ns for cold plasma runs and 9.15 ns for extremely relativistic plasma runs, which are respectively 81 and 27 times faster than a single threaded state-of-art \{CPU\} code. A particle-based computation thread assignment was used in the current deposition scheme and write conflicts among the threads were resolved by a thread racing technique. A parallel particle sorting scheme was also developed and used. The implementation took advantage of fast on-chip shared memory, and can in principle be extended to 3D. {
}

@article{Trefethen2013,
title = {Energy-aware software: Challenges, opportunities and strategies },
journal = {Journal of Computational Science },
volume = {4},
number = {6},
pages = {444 - 449},
year = {2013},
note = {Scalable Algorithms for Large-Scale Systems Workshop (ScalA2011), Supercomputing 2011 },
issn = {1877-7503},
doi = {http://dx.doi.org/10.1016/j.jocs.2013.01.005},
url = {http://www.sciencedirect.com/science/article/pii/S1877750313000173},
author = {Anne E. Trefethen and Jeyarajan Thiyagalingam},
abstract = {Energy consumption of computing systems has become a major concern. Constrained by cost, environmental concerns and policy, minimising the energy foot-print of computing systems is one of the primary goals of many initiatives. As we move towards exascale computing, energy constraints become very real and are a major driver in design decisions. The issue is also apparent at the scale of desk top machines, where many core and accelerator chips are common and offer a spectrum of opportunities for balancing energy and performance. Conventionally, approaches for reducing energy consumption have been either at the operational level (such as powering down all or part of systems) or at the hardware design level (such as utilising specialised low-energy components). In this paper, we are interested in a different approach; energy-aware software. By measuring the energy consumption of a computer application and understanding where the energy usage lies, may allow a change of the software to provide opportunities for energy savings. In order to understand the complexities of this approach, we specifically look at multithreaded algorithms and applications. By an evaluation of a benchmark suite on multiple architectures and multiple environments, we show how basic parameters, such as threading options, compilers and frequencies, can impact energy consumption. As such, we provide an overview of the challenges that face software developers in this regard. We then offer a view of the directions that need to be taken and possible strategies needed for building energy-aware software. {
}

@article{Gicquel2012782,
title = {Large Eddy Simulations of gaseous flames in gas turbine combustion chambers },
journal = {Progress in Energy and Combustion Science },
volume = {38},
number = {6},
pages = {782 - 817},
year = {2012},
note = {},
issn = {0360-1285},
doi = {http://dx.doi.org/10.1016/j.pecs.2012.04.004},
url = {http://www.sciencedirect.com/science/article/pii/S0360128512000366},
author = {L.Y.M. Gicquel and G. Staffelbach and T. Poinsot},
keywords = {Large Eddy Simulations},
keywords = {Complex geometry},
keywords = {Swirled flows},
keywords = {Gaseous combustion},
keywords = {Turbulent combustion},
keywords = {Gas turbine },
abstract = {Recent developments in numerical schemes, turbulent combustion models and the regular increase of computing power allow Large Eddy Simulation (LES) to be applied to real industrial burners. In this paper, two types of \{LES\} in complex geometry combustors and of specific interest for aeronautical gas turbine burners are reviewed: (1) laboratory-scale combustors, without compressor or turbine, in which advanced measurements are possible and (2) combustion chambers of existing engines operated in realistic operating conditions. Laboratory-scale burners are designed to assess modeling and fundamental flow aspects in controlled configurations. They are necessary to gauge \{LES\} strategies and identify potential limitations. In specific circumstances, they even offer near model-free or DNS-like \{LES\} computations. \{LES\} in real engines illustrate the potential of the approach in the context of industrial burners but are more difficult to validate due to the limited set of available measurements. Usual approaches for turbulence and combustion sub-grid models including chemistry modeling are first recalled. Limiting cases and range of validity of the models are specifically recalled before a discussion on the numerical breakthrough which have allowed \{LES\} to be applied to these complex cases. Specific issues linked to real gas turbine chambers are discussed: multi-perforation, complex acoustic impedances at inlet and outlet, annular chambers…. Examples are provided for mean flow predictions (velocity, temperature and species) as well as unsteady mechanisms (quenching, ignition, combustion instabilities). Finally, potential perspectives are proposed to further improve the use of \{LES\} for real gas turbine combustor designs. {
}

@article{Subotić2013,
title = {Programmability and portability for exascale: Top down programming methodology and tools with StarSs },
journal = {Journal of Computational Science },
volume = {4},
number = {6},
pages = {450 - 456},
year = {2013},
note = {Scalable Algorithms for Large-Scale Systems Workshop (ScalA2011), Supercomputing 2011 },
issn = {1877-7503},
doi = {http://dx.doi.org/10.1016/j.jocs.2013.01.008},
url = {http://www.sciencedirect.com/science/article/pii/S1877750313000203},
author = {Vladimir Subotić and Steffen Brinkmann and Vladimir Marjanović and Rosa M. Badia and Jose Gracia and Christoph Niethammer and Eduard Ayguade and Jesus Labarta and Mateo Valero},
keywords = {Parallel programming models},
keywords = {Performance analysis tools},
keywords = {Development tools },
abstract = {StarSs is a task-based programming model that allows to parallelize sequential applications by means of annotating the code with compiler directives. The model further supports transparent execution of designated tasks on heterogeneous platforms, including clusters of GPUs. This paper focuses on the methodology and tools that complements the programming model forming a consistent development environment with the objective of simplifying the live of application developers. The programming environment includes the tools \{TAREADOR\} and TEMANEJO, which have been designed specifically for StarSs. TAREADOR, a Valgrind-based tool, allows a top-down development approach by assisting the programmer in identifying tasks and their data-dependencies across all concurrency levels of an application. \{TEMANEJO\} is a graphical debugger supporting the programmer by visualizing the task dependency tree on one hand, but also allowing to manipulate task scheduling or dependencies. These tools are complemented with a set of performance analysis tools (Scalasca, Cube and Paraver) that enable to fine tune StarSs application. {
}

@article{Dongarra2015,
title = {HPC Programming on Intel Many-Integrated-Core Hardware with MAGMA Port to Xeon Phi},
journal = {Scientific Programming},
volume = {2015},
year = {2015},
author = {Jack Dongarra Mark Gates Azzam Haidar Yulu Jia Khairul Kabir Piotr Luszczek and Stanimire Tomov},
}

@article{Reed2015,
title = {Exascale Computing and Big Data},
journal = {Communications of the ACM},
volume = {58},
number = {7},
pages = {56-68},
year = {2015},
author = {Reed, Daniel A., and Dongarra J.},
}

@article{Vázquez2016,
title = {Alya: Multiphysics engineering simulation toward exascale },
journal = {Journal of Computational Science },
volume = {},
number = {},
pages = { - },
year = {2016},
note = {},
issn = {1877-7503},
doi = {http://dx.doi.org/10.1016/j.jocs.2015.12.007},
url = {http://www.sciencedirect.com/science/article/pii/S1877750315300521},
author = {Mariano Vázquez and Guillaume Houzeaux and Seid Koric and Antoni Artigues and Jazmin Aguado-Sierra and Ruth Arís and Daniel Mira and Hadrien Calmet and Fernando Cucchietti and Herbert Owen and Ahmed Taha and Evan Dering Burness and José María Cela and Mateo Valero},
keywords = {Multi-physics coupling},
keywords = {Parallelization},
keywords = {Computational mechanics },
abstract = {Abstract Alya is a multi-physics simulation code developed at Barcelona Supercomputing Center (BSC). From its inception Alya code is designed using advanced High Performance Computing programming techniques to solve coupled problems on supercomputers efficiently. The target domain is engineering, with all its particular features: complex geometries and unstructured meshes, coupled multi-physics with exotic coupling schemes and physical models, ill-posed problems, flexibility needs for rapidly including new models, etc. Since its beginnings in 2004, Alya has scaled well in an increasing number of processors when solving single-physics problems such as fluid mechanics, solid mechanics, acoustics, etc. Over time, we have made a concerted effort to maintain and even improve scalability for multi-physics problems. This poses challenges on multiple fronts, including: numerical models, parallel implementation, physical coupling models, algorithms and solution schemes, meshing process, etc. In this paper, we introduce Alya's main features and focus particularly on its solvers. We present Alya's performance up to 100.000 processors in Blue Waters, the \{NCSA\} supercomputer with selected multi-physics tests that are representative of the engineering world. The tests are incompressible flow in a human respiratory system, low Mach combustion problem in a kiln furnace, and coupled electro-mechanical contraction of the heart. We show scalability plots for all cases and discuss all aspects of such simulations, including solver convergence. {
}

@article{Engelmann2014,
title = {Scaling to a million cores and beyond: Using light-weight simulation to understand the challenges ahead on the road to exascale },
journal = {Future Generation Computer Systems },
volume = {30},
number = {},
pages = {59 - 65},
year = {2014},
note = {Special Issue on Extreme Scale Parallel Architectures and Systems, Cryptography in Cloud Computing and Recent Advances in Parallel and Distributed Systems, \{ICPADS\} 2012 Selected Papers },
issn = {0167-739X},
doi = {http://dx.doi.org/10.1016/j.future.2013.04.014},
url = {http://www.sciencedirect.com/science/article/pii/S0167739X13000745},
author = {Christian Engelmann},
keywords = {Parallel discrete event simulation},
keywords = {Message passing interface},
keywords = {Collective communication},
keywords = {High performance computing},
keywords = {Exascale },
abstract = {Abstract As supercomputers scale to 1000 PFlop/s over the next decade, investigating the performance of parallel applications at scale on future architectures and the performance impact of different architecture choices for high-performance computing (HPC) hardware/software co-design is crucial. This paper summarizes recent efforts in designing and implementing a novel \{HPC\} hardware/software co-design toolkit. The presented Extreme-scale Simulator (xSim) permits running an \{HPC\} application in a controlled environment with millions of concurrent execution threads while observing its performance in a simulated extreme-scale \{HPC\} system using architectural models and virtual timing. This paper demonstrates the capabilities and usefulness of the xSim performance investigation toolkit, such as its scalability to 227 simulated Message Passing Interface (MPI) ranks on 960 real processor cores, the capability to evaluate the performance of different \{MPI\} collective communication algorithms, and the ability to evaluate the performance of a basic Monte Carlo application with different architectural parameters. {
}

@article{Dosanjh2014,
title = {Exascale design space exploration and co-design },
journal = {Future Generation Computer Systems },
volume = {30},
number = {},
pages = {46 - 58},
year = {2014},
note = {Special Issue on Extreme Scale Parallel Architectures and Systems, Cryptography in Cloud Computing and Recent Advances in Parallel and Distributed Systems, \{ICPADS\} 2012 Selected Papers },
issn = {0167-739X},
doi = {http://dx.doi.org/10.1016/j.future.2013.04.018},
url = {http://www.sciencedirect.com/science/article/pii/S0167739X13000782},
author = {S.S. Dosanjh and R.F. Barrett and D.W. Doerfler and S.D. Hammond and K.S. Hemmert and M.A. Heroux and P.T. Lin and K.T. Pedretti and A.F. Rodrigues and T.G. Trucano and J.P. Luitjens},
keywords = {High performance computing},
keywords = {Scientific computing},
keywords = {Co-design},
keywords = {Exascale preparation },
abstract = {Abstract The co-design of architectures and algorithms has been postulated as a strategy for achieving Exascale computing in this decade. Exascale design space exploration is prohibitively expensive, at least partially due to the size and complexity of scientific applications of interest. Application codes can contain millions of lines and involve many libraries. Mini-applications, which attempt to capture some key performance issues, can potentially reduce the order of the exploration by a factor of a thousand. However, we need to carefully understand how representative mini-applications are of the full application code. This paper describes a methodology for this comparison and applies it to a particularly challenging mini-application. A multi-faceted methodology for design space exploration is also described that includes measurements on advanced architecture testbeds, experiments that use supercomputers and system software to emulate future hardware, and hardware/software co-simulation tools to predict the behavior of applications on hardware that does not yet exist. {
}

@article{Norman2015,
title = {Developing A Large Time Step, Robust, and Low Communication Multi-Moment \{PDE\} Integration Scheme for Exascale Applications },
journal = {Procedia Computer Science },
volume = {51},
number = {},
pages = {1848 - 1857},
year = {2015},
note = {International Conference On Computational Science, \{ICCS\} 2015Computational Science at the Gates of Nature },
issn = {1877-0509},
doi = {http://dx.doi.org/10.1016/j.procs.2015.05.413},
url = {http://www.sciencedirect.com/science/article/pii/S1877050915012211},
author = {Matthew R. Norman},
keywords = {MCV},
keywords = {Multi-moment},
keywords = {Finite-volume},
keywords = {ADER},
keywords = {Transport },
abstract = {Abstract The Boundary Averaged Multi-moment Constrained finite-Volume (BA-MCV) method is de- rived, explained, and evaluated for 1-D transport to assess accuracy, maximum stable time step (MSTS), oscillations for discontinuous data, and parallel communication burden. The BA-MCV scheme is altered from the original \{MCV\} scheme to compute the updates of point wise cell boundary derivatives entirely locally. Then it is altered such that boundary moments are replaced with the interface upwind value. The scheme is stable at a maximum stable \{CFL\} (MSCFL) value of one no matter how high-order the scheme is, giving significantly larger time steps than Galerkin methods, for which the \{MSCFL\} decreases nearly quadratically with in- creasing order. The BA-MCV method is compared against a \{SE\} method at varying order, both using the ADER-DT time discretization. BA-MCV error for a sine wave was comparable to the same order of accuracy for a \{SE\} method. The resulting large time step, multi-moment, low communication scheme is of great interest for exascale architectures. {
}

@incollection{Talia2016,
title = {Chapter 5 - Research Trends in Big Data Analysis },
editor = {Marozzo, Domenico TaliaPaolo TrunfioFabrizio },
booktitle = {Data Analysis in the Cloud },
publisher = {Elsevier},
edition = {},
address = {Boston},
year = {2016},
pages = {123 - 138},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-802881-0},
doi = {http://dx.doi.org/10.1016/B978-0-12-802881-0.00005-6},
url = {http://www.sciencedirect.com/science/article/pii/B9780128028810000056},
author = {Domenico Talia and Paolo Trunfio and Fabrizio Marozzo},
keywords = {Data-intensive exascale computing},
keywords = {manycore systems},
keywords = {exascale computing},
keywords = {social network analysis},
keywords = {urban computing},
keywords = {trajectory mining},
keywords = {research trends},
keywords = {in-memory data analysis },
abstract = {Abstract Big data analysis is a very active research area with significant impact on industrial and scientific domains where is important to analyze very large and complex data repositories. In particular, in many cases data to be analyzed are stored in cloud platforms and elastic computing clouds facilities are exploited to speedup the analysis. This chapter outlines and discusses main research trends in big data analytics and cloud systems for managing and mining large-scale data repositories. Topics and trends in the areas of exascale computing and social data analysis are reported. Section 5.1 discusses issues and challenges for implementing massively parallel and/or distributed applications in the area of big data analysis on exascale systems. Section 5.2 discusses recent trends in social data analysis, with a focus on mining mobility patterns from large volumes of trajectory data from online social network data. Finally, Section 5.3 discusses key research areas for the implementation of scalable data analytics dealing with huge, distributed data sources. {
}

@article{Zounmevo2014,
title = {A fast and resource-conscious \{MPI\} message queue mechanism for large-scale jobs },
journal = {Future Generation Computer Systems },
volume = {30},
number = {},
pages = {265 - 290},
year = {2014},
note = {Special Issue on Extreme Scale Parallel Architectures and Systems, Cryptography in Cloud Computing and Recent Advances in Parallel and Distributed Systems, \{ICPADS\} 2012 Selected Papers },
issn = {0167-739X},
doi = {http://dx.doi.org/10.1016/j.future.2013.07.003},
url = {http://www.sciencedirect.com/science/article/pii/S0167739X13001489},
author = {Judicael A. Zounmevo and Ahmad Afsahi},
keywords = {MPI},
keywords = {Message queues},
keywords = {Multidimensional searches},
keywords = {Scalability},
keywords = {Exascale },
abstract = {Abstract The Message Passing Interface (MPI) message queues have been shown to grow proportionately to the job size for many applications. With such a behaviour and knowing that message queues are used very frequently, ensuring fast queue operations at large scales is of paramount importance in the current and the upcoming exascale computing eras. Scalability, however, is two-fold. With the growing processor core density per node, and the expected smaller memory density per core at larger scales, a queue mechanism that is blind on memory requirements poses another scalability issue even if it solves the speed of operation problem. In this work we propose a multidimensional queue management mechanism whose operation time and memory overhead grow sub-linearly with the job size. We show why a novel approach is justified in spite of the existence of well-known and fast data structures such as binary search trees. We compare our proposal with a linked list-based approach which is not scalable in terms of speed of operation, and with an array-based method which is not scalable in terms of memory consumption. Our proposed multidimensional approach yields queue operation time speedups that translate to up to 4-fold execution time improvement over the linked list design for the applications studied in this work. It also shows a consistent lower memory footprint compared to the array-based design. Finally, compared to the linked list-based queue, our proposed design yields cache miss rate improvements which are on average on par with the array-based design. {
}

@article{Mosby201668,
title = {Computational homogenization at extreme scales },
journal = {Extreme Mechanics Letters },
volume = {6},
number = {},
pages = {68 - 74},
year = {2016},
note = {},
issn = {2352-4316},
doi = {http://dx.doi.org/10.1016/j.eml.2015.12.009},
url = {http://www.sciencedirect.com/science/article/pii/S2352431615300134},
author = {Matthew Mosby and Karel Matouš},
keywords = {Computational homogenization},
keywords = {High-performance computing},
keywords = {Extreme scale computing},
keywords = {Heterogeneous interfaces},
keywords = {Multi-scale interfacial damage modeling },
abstract = {Abstract Multi-scale simulations at extreme scales in terms of both physical length scales and computational resources are presented. In this letter, we introduce a hierarchically parallel computational homogenization solver that employs hundreds of thousands of computing cores and resolves O ( 10 5 ) in material length scales (from O ( cm ) to O ( 100 nm ) ). Simulations of this kind are important in understanding the multi-scale essence of many natural and synthetically made materials. Thus, we present a simulation consisting of 53.8 Billion finite elements with 28.1 Billion nonlinear equations that is solved on 393,216 computing cores (786,432 threads). The excellent parallel performance of the computational homogenization solver is demonstrated by a strong scaling test from 4,096 to 262,144 cores. A fully coupled multi-scale damage simulation shows a complex crack profile at the micro-scale and the macroscopic crack tunneling phenomenon. Such large and predictive simulations are an important step towards Virtual Materials Testing and can aid in development of new material formulations with extreme properties. Furthermore, the high computational efficiency of our computational homogenization solver holds great promise for utilizing the next generation of exascale parallel computing platforms that are expected to accelerate computations through orders of magnitude increase in parallelism rather than speed of each processor. {
}

@article{Nakashima201581,
title = {Manycore challenge in particle-in-cell simulation: How to exploit 1 \{TFlops\} peak performance for simulation codes with irregular computation },
journal = {Computers & Electrical Engineering },
volume = {46},
number = {},
pages = {81 - 94},
year = {2015},
note = {},
issn = {0045-7906},
doi = {http://dx.doi.org/10.1016/j.compeleceng.2015.03.010},
url = {http://www.sciencedirect.com/science/article/pii/S004579061500097X},
author = {Hiroshi Nakashima},
keywords = {Manycore processors},
keywords = {SIMD-vectorization},
keywords = {Multithreading},
keywords = {Particle-in-cell simulation},
keywords = {High-performance computing },
abstract = {Abstract This paper discusses the challenge in post-Peta and Exascale era especially that brought by manycore processors of ordinary (i.e., non-GPU type) \{CPU\} cores. Though such a processor like Intel Xeon Phi gives us TFlops-class computational power and may lead us to Exascale computing, full exploitation of its potential is far from an easy job due to its source of high performance, namely a large scale multithreading and a wide \{SIMD\} mechanism. In fact, in the three-tier parallelism namely inter-node, intra-node and intra-core ones, we found their order does not represent the toughness in \{HPC\} programming but the order should be reversed to do that. Our case study with a particle-in-cell plasma simulation code supports our observation revealing that a simple porting of an existing code to Xeon Phi is infeasible from the viewpoint of performance and we have to make a significant change of the code structure so that it conforms with the features of the processor. However the study also confirms that the recoding effort is well rewarded achieving a good single-node performance higher than that obtained from an execution on four dual-socket nodes of Cray XE6. {
}

@article{Yu2015,
title = {Quantitative modeling of power performance tradeoffs on extreme scale systems },
journal = {Journal of Parallel and Distributed Computing },
volume = {84},
number = {},
pages = {1 - 14},
year = {2015},
note = {},
issn = {0743-7315},
doi = {http://dx.doi.org/10.1016/j.jpdc.2015.06.006},
url = {http://www.sciencedirect.com/science/article/pii/S0743731515001045},
author = {Li Yu and Zhou Zhou and Sean Wallace and Michael E. Papka and Zhiling Lan},
keywords = {High performance computing},
keywords = {Power performance analysis},
keywords = {Colored Petri net},
keywords = {Extreme scale systems},
keywords = {Power capping },
abstract = {Abstract As high performance computing (HPC) continues to grow in scale and complexity, energy becomes a critical constraint in the race to exascale computing. The days of “performance at all cost” are coming to an end. While performance is still a major objective, future \{HPC\} will have to deliver desired performance under the energy constraint. Among various power management methods, power capping is a widely used approach. Unfortunately, the impact of power capping on system performance, user jobs, and power-performance efficiency are not well studied due to many interfering factors imposed by system workload and configurations. To fully understand power management in extreme scale systems with a fixed power budget, we introduce a power-performance modeling tool named PuPPET (Power Performance \{PETri\} net). Unlike the traditional performance modeling approaches such as analytical methods or trace-based simulators, we explore a new approach–colored Petri nets–for the design of PuPPET. PuPPET is fast and extensible for navigating through different configurations. More importantly, it can scale to hundreds of thousands of processor cores and at the same time provide high levels of modeling accuracy. We validate PuPPET by using system traces (i.e., workload log and power data) collected from the production 48-rack \{IBM\} Blue Gene/Q supercomputer at Argonne National Laboratory. Our trace-based validation demonstrates that PuPPET is capable of modeling the dynamic execution of parallel jobs on the machine by providing an accurate approximation of energy consumption. In addition, we present two case studies of using PuPPET to study power-performance tradeoffs on petascale systems. {
}

@article{Sitaraman2016,
title = {Balancing conflicting requirements for grid and particle decomposition in continuum-Lagrangian solvers },
journal = {Parallel Computing },
volume = {52},
number = {},
pages = {1 - 21},
year = {2016},
note = {},
issn = {0167-8191},
doi = {http://dx.doi.org/10.1016/j.parco.2015.10.010},
url = {http://www.sciencedirect.com/science/article/pii/S0167819115001428},
author = {Hariswaran Sitaraman and Ray Grout},
keywords = {Load balancing},
keywords = {Lagrangian particle tracking},
keywords = {Particle in cell},
keywords = {Exascale simulations },
abstract = {Abstract Load balancing strategies for hybrid solvers that involve grid based partial differential equation solution coupled with particle tracking are presented in this paper. A typical Message Passing Interface (MPI) based parallelization of grid based solves are done using a spatial domain decomposition while particle tracking is primarily done using either of the two techniques. One of the techniques is to distribute the particles to \{MPI\} ranks to whose grid they belong to while the other is to share the particles equally among all ranks, irrespective of their spatial location. The former technique provides spatial locality for field interpolation but cannot assure load balance in terms of number of particles, which is achieved by the latter. The two techniques are compared for a case of particle tracking in a homogeneous isotropic turbulence box as well as a turbulent jet case. A strong scaling study is performed to more than 32,000 cores, which results in particle densities representative of anticipated exascale machines. The use of alternative implementations of \{MPI\} collectives and efficient load equalization strategies are studied to reduce data communication overheads. {
}

@article{Wuyts2015,
title = {Helsim: A Particle-in-cell Simulator for Highly Imbalanced Particle Distributions },
journal = {Procedia Computer Science },
volume = {51},
number = {},
pages = {2923 - 2927},
year = {2015},
note = {International Conference On Computational Science, \{ICCS\} 2015Computational Science at the Gates of Nature },
issn = {1877-0509},
doi = {http://dx.doi.org/10.1016/j.procs.2015.05.479},
url = {http://www.sciencedirect.com/science/article/pii/S1877050915012879},
author = {Roel Wuyts and Tom Haber and Giovanni Lapenta},
keywords = {Particle-in-cell},
keywords = {Computational Science},
keywords = {High Performance Computing },
abstract = {Abstract Helsim is a 3D electro-magnetic particle-in-cell simulator used to simulate the behaviour of plasma in space. Particle-in-cell simulators track the movement of particles through space, with the particles generating and being subjected to various fields (electric, magnetic and or gravitational). Helsim dissociates the particles data structure from the fields, allowing them to be distributed and load- balanced independently and can simulate experiments with highly im-balanced particle distributions with ease. This paper shows weak scaling results of a highly im-balanced particle setup on up to 32 thousand cores. The results validate the basic claims for scal-ability for imbalanced particle distributions, but also highlights a problem with a workaround we had to implement to circumvent an OpenMPI bug we encountered. {
}

@article{Tucker2016,
title = {Eddy resolving simulations in aerospace – Invited paper (Numerical Fluid 2014) },
journal = {Applied Mathematics and Computation },
volume = {272, Part 3},
number = {},
pages = {582 - 592},
year = {2016},
note = {The 9th International Symposium on Numerical Analysis of Fluid Flow and Heat Transfer - Numerical Fluids 2014 },
issn = {0096-3003},
doi = {http://dx.doi.org/10.1016/j.amc.2015.02.018},
url = {http://www.sciencedirect.com/science/article/pii/S009630031500171X},
author = {Paul G. Tucker and James C. Tyacke},
keywords = {LES},
keywords = {DES},
keywords = {Turbulence},
keywords = {Aerospace},
keywords = {Numerical methods },
abstract = {Abstract The future use of eddy resolving simulations (ERS) such as Large Eddy Simulation (LES), Direct Numerical Simulation (DNS) and related approaches in aerospace is explored. The turbulence modeling requirements with respect to aeroengines and aircraft is contrasted. For the latter, higher Reynolds numbers are more prevalent and this especially gives rise to the need for the hybridization of \{ERS\} methods with Reynolds Averaged Navier–Stokes (RANS) approaches. Zones where future use of pure \{ERS\} methods is now possible and those where hybridizations with \{RANS\} will be needed is outlined. The major focus is the aeroengine for which the component scales are much smaller. This gives rise to generally more benign Reynolds numbers. The use of eddy resolving methods in a wide range of zones in an aeroengine is discussed and the potential benefits and also cost drawbacks with such approaches noted. The tension when using such computationally intensive calculations in an area where the coupling of components and even the airframe and engine is becoming increasingly important is explored. Also, the numerical methods and meshing requirements are considered and the implications of \{ERS\} methods for future numerical algorithms. It is postulated that such simulations are ready now for niche uses in industry. However, to perform the scale of simulations that industry requires, to meet pressing environmental needs, challenges remain. For example, there is the need to develop optimal numerical methods that both map to the accuracy requirements for \{ERS\} and also future computer architectures. {
}

@article{Collins2015,
title = {Progress in Fast, Accurate Multi-scale Climate Simulations },
journal = {Procedia Computer Science },
volume = {51},
number = {},
pages = {2006 - 2015},
year = {2015},
note = {International Conference On Computational Science, \{ICCS\} 2015Computational Science at the Gates of Nature },
issn = {1877-0509},
doi = {http://dx.doi.org/10.1016/j.procs.2015.05.465},
url = {http://www.sciencedirect.com/science/article/pii/S1877050915012739},
author = {W.D. Collins and H. Johansen and K.J. Evans and C.S. Woodward and P.M. Caldwell},
keywords = {Earth system models},
keywords = {Multi-scale climate},
keywords = {Time integration},
keywords = {Many-core },
abstract = {Abstract We present a survey of physical and computational techniques that have the potential to contribute to the next generation of high-fidelity, multi-scale climate simulations. Examples of the climate science problems that can be investigated with more depth with these computational improvements include the capture of remote forcings of localized hydrological extreme events, an accurate representation of cloud features over a range of spatial and temporal scales, and parallel, large ensembles of simulations to more effectively explore model sensitivities and un- certainties. Numerical techniques, such as adaptive mesh refinement, implicit time integration, and separate treatment of fast physical time scales are enabling improved accuracy and fidelity in simulation of dynamics and allowing more complete representations of climate features at the global scale. At the same time, partnerships with computer science teams have focused on taking advantage of evolving computer architectures such as many-core processors and GPUs. As a result, approaches which were previously considered prohibitively costly have become both more efficient and scalable. In combination, progress in these three critical areas is poised to transform climate modeling in the coming decades. These topics have been presented within a workshop titled, “Numerical and Computational Developments to Advance Multiscale Earth System Models (MSESM ‘15),” as part of the International Conference on Computational Sciences, Reykjavik, Iceland, June 1-3, 2015. {
}

@article{Löhner201353,
title = {Handling tens of thousands of cores with industrial/legacy codes: Approaches, implementation and timings },
journal = {Computers & Fluids },
volume = {85},
number = {},
pages = {53 - 62},
year = {2013},
note = {International Workshop on Future of \{CFD\} and Aerospace Sciences },
issn = {0045-7930},
doi = {http://dx.doi.org/10.1016/j.compfluid.2012.09.030},
url = {http://www.sciencedirect.com/science/article/pii/S0045793012004112},
author = {Rainald Löhner and Joseph D. Baum},
keywords = {Massive parallelism},
keywords = {OpenMP},
keywords = {MPI},
keywords = {GPUs},
keywords = {CFD },
abstract = {Abstract The consequences that the recent stagnation in clockrates for \{CPUs\} has had on large-scale \{CFD\} runs are examined. At first sight, the conclusion is that only massive parallelism at the loop or domain decomposition level will lead to higher \{FLOP\} counts. However, the significant differences in advances for CPUs/GPUs versus \{RAM\} and interprocessor communication bandwidth lead to a so-called ‘limiting domain size’, below which communication dominates execution times and performance degrades drastically. The consequences of this ‘red-shift’ for the future of \{CFD\} are manifold: the time to advance the solution one timestep is limited, implying that even with unlimited number of processors/cores, LES, \{DES\} and \{DNS\} runs for realistic Reynolds-numbers will require days or weeks of execution. {
}
























