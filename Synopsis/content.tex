
\section*{Общая характеристика работы}

\newcommand{\actuality}{\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\underline{\textbf{\tasksTXT}}}
\newcommand{\novelty}{\underline{\textbf{\noveltyTXT}}}
\newcommand{\influence}{\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов ...

\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
пяти глав, заключения и~приложения. Полный объем диссертации
\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список литературы содержит \textbf{ХХX}~наименование.

\section*{Содержание работы}
Во \underline{\textbf{введении}} 
обоснована актуальность темы исследования и степень
ее разработанности, сформулированы цели и задачи работы, показана ее научная новизна, теоретическая и практическая значимость; представлены положения, выносимые на защиту, а также степень достоверности и апробация результатов.






\underline{\textbf{Первая глава}} посвящена описанию созданной в рамках диссертационной работы реализация метода частиц в ячейках на параллельных ВС с использованием различных типов ускорителей вычислений. 

\textbf{Описание программы для моделирования методом частиц в ячейках.}
%В \textit{первом разделе} описана реализованная в рамках диссертационной работы программа для моделирования плазмы методом частиц в ячейках. 
%С точки зрения построения теста область приложения не имеет решающего значения, точно так же можно было бы использовать гидро-газодинамические расчеты по методу частиц, или астрофизические приложения. 

Параллельная программа, реализующая  метод частиц в ячейках именно в физике плазмы характеризуется наибольшей из всех возможных моделей сплошной среды степенью неопределенности по нагрузке на отдельные узлы вычислительной системы и на подситему ввода вывода, что делает именно такие приложения идеальным тестом для высокопроизводительных вычислительных систем. 

Метод частиц в ячейках применяется в данной работе для решения системы уравнений Власова-Максвелла, описывающих динамику бесстолкновительной плазмы.
Реализация в основном соответствует описанному в книге \cite{VshivkovPICbook}
математическая модель высокотемпературной бесстолкновительной плазмы.
Модель состоит из кинетического уравнения Власова и системы
уравнений Максвелла, которые в безразмерной форме имеют
следующий вид:

\begin{equation}\label{eq:Vlas}
\frac{\partial f_{i,e}}{\partial t}+{\bf{v}} \frac{\partial f_{i,e}}{\partial \bf{r}}+q_{i,e}({\bf{E}}+[{\bf{v}},{\bf{B}}])\frac{\partial f_{i,e}}{\partial \bf{v}}=0, 
\end{equation}

\begin{equation}
\begin{array}{c}\displaystyle

\frac{\partial \bf{E}}{\partial t}=\nabla\times \bf{B} - \bf{j},  \label{eq:dE}
\\[5mm]\displaystyle
\frac{\partial \bf{B}}{\partial t}=-\nabla\times \bf{E},  \label{eq:dB}
\\[5mm]\displaystyle
\nabla \bf{E} = \rho, \label{eq:divE}
\\[5mm]\displaystyle
\nabla \bf{B} = 0.  \label{eq:divB}

\end{array}
\end{equation}

Здесь индексами $i$ и $e$ помечены величины, относящиеся к ионам и
электронам, со\-от\-ветст\-вен\-но; $q_e=-1, \quad q_i=m_e/m_i$; $f_{i,e}(t,\bf{r},\bf{p})$ --
функция распределения частиц; $m_{i,e}, \bf{p}_{i,e},
\bf{r}_{i,e}$ -- масса, импульс, положение иона или электрона;
$\bf{E}$, $\bf{B}$ -- напряженности электрического и магнитного
полей. 

Для перехода к безразмерному виду в качестве единиц
используются следующие базовые величины(Месяц, Снытников, Лотов, 2013):
\begin{itemize}
	\item скорость света;
	\item масса электрона;
	\item плотность плазмы $n_0=10^{14}$ см $^{-3}$;
	\item время $t=\omega _{pe}^{-1}$, где плазменная электронная частота $\omega_{pe} =5,6 \cdot 10^{11}$c$^{-1}$.
\end{itemize}

Использование метода частиц в ячейках означает формально-математический переход от уравнения Власова к набору его уравнений характеристик, которые является уравнениями движения модельных частиц.

Таким образом, в методе частиц в ячейках решаются два типа уравнений: уравнения движения модельных частиц и уравнения Власова. Необходимо кратко рассмотреть особенности этих уравнений, существенные с точки зрения создания программы и последующего построения теста на ее основе.

Для решения уравнений движения модельных частиц основных особенностей две: большой объем вычислений: от $10^6$ до $10^9$
модельных частиц при количестве операций с плавающей точкой от 200 до 500 на одну частицу и нерегулярный характер доступа к памяти, вызванный необходимостью обращения к трехмерных массивам электромагнитных полей для расчета сдвига каждой частицы, которыерасположены в пространстве произвольных образом. Кроме того, при параллельной реализациии метода частиц, и связанной с этим декомпозиции расчетной области, нерегулярный характер имеют также пересылки модельных частиц между узлами высокопроизводительной ВС.

Для решения уравнений Максвелла доступ к памяти, напротив, является регулярным, объем данных на 2-3 порядка меньше. 

%Важной частью программы являются процедуры, вычисляющие различные физические диагностики: преобразование Фурье плотностей и токов, векторные поля скоростей частиц, просто выдачи величин, имеющихся в программе: электромагнитных полей, токов, координат и импульсов частиц. Эти операции характеризуются большим объемом (одна частица в трехмерном моделировании занимает 48 байт) и нерегулярностью.

Таким образом основные сложности в реализации метода частиц, и одновременно особенности программы, позволяющие ее использовать в качестве теста для производительности ВС, связаны с расчетом движения модельных частиц.% и с операциями ввода-вывода.

Приведем \textbf{общую схему вычислений в программе}:
\begin{enumerate}
	\item Создание начального 
	распределения модельных частиц
	(выполняется независимо 
	всеми MPI-процессами) (\textbf{координаты и импульсы частиц генерируются в оперативной памяти} ). 
	\item Далее на каждом временном шаге:
	\begin{enumerate}
		\item Вычисление полей. Обмен граничными значениями полей между соседними узлами многопроцессорной ВС
		\item Вычисление сдвига модельных частиц. Вычисление токов, обмен граничными значениями токов между соседними узлами многопроцессорной ВС
		\item Выполнение диагностических процедур, вывод результатов на диск. Вывод выполняется стандартными средствами C/C++.
	\end{enumerate}	
\end{enumerate}

Приведем также
\textbf{описание выходных данных программы.}
Результатами работы являются
(опционально, каждая выдача может быть отключена):
\begin{itemize}
	\item Списки модельных частиц (координаты, импульсы)
	\item Плотности электронов, ионов и электронов пучка
	\item Электрическое и магнитное поля (все три компоненты)
	\item Ток (три компоненты)
	\item Одномерная функция распределения электронов (всех, и пучка, и плазмы) по энергии
\end{itemize}

С точки зрения тестирования ВС основное значение имеют следующие выдачи (отдельно для каждого MPI-процесса):

\begin{itemize}
	\item Средняя продолжительность операций MPI, причем по отдельности для разных объемов пересылаемых данных и разных стадий вычислительного алгоритма:
\begin{itemize}
	\item MPI\_Send/Recv - пересылка граничных значений поля и пересылка частиц
	\item MPI\_Allreduce - сложение значений тока в подобласти при лагранжевой декомпозиции
	
\end{itemize}	
	\item Время пересылки частиц, граничных значений поля, время сборки данных при выполнении коллективных операций   
	\item Время выполнения каждой отдельной стадии вычислительного алгоритма:
\begin{itemize}
	\item расчет поля, 
	\item пересылка граничных значений, 
	\item расчет движения частиц, 
	\item пересылка частиц в соседние процессоры, 
	\item сборка значений тока при лагранжевой декомпозии
\end{itemize}
	\item Время записи файлов на диск 
	\item Время копирования данных на GPU и обратно
	
\end{itemize}
Расчет производительности ВС в целом и отдельных ее элементов выполняется после выполнения программы, на основе полученных времен. 

Для ВС гибридной архитектуры - на основе графических ускорителей и ускорителей Intel Xeon Phi реализован следующий вариант: на узле ВС данного типа запускается более одного MPI-процесса в том случае, когда в состав узла входит более одного GPU или Intel Xeon Phi, т.е число процессов на узел равно числу ускорителей.

Реализованная в диссертационной работе тестовая программа получила условное наименование 
PIC-MANAS (Particle-In-Cell Multi-Archtitecture Numerical Analysis & Simulation).



\underline{\textbf{Вторая глава}} содержит проведенный обзор литературы по вычислениям, проводимых на современных петафлопсных и перспективных экзафлопсных ВС. В частности выявлены вычислительные алгоритмы, которым уделяется особенное внимание применительно к вычислениям на таких ВС.
В обзорную часть данной диссертационной работы вошло 298 статей за период с 2010 по 2016 год в следующих журналах:Future Generation Computer Systems, Procedia Computers Science, Journal of Parallel and Distributed Computing, Parallel Computing, Journal of Computational Physics, Computer Physics Communications и др.  Распределение статей по приложениям показано в таблице \ref{tab_physics}.

\begin{table}[ht]
	\caption{Распределение по приложениям статей относящихся к тематике <<экзафлопсные вычисления>>}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			вычислительная гидродинамика & 27  \\ \hline 
			ядерные технологии & 17       \\ \hline  
			физика плазмы & 11  \\ \hline 
			разработка новых материалов & 10  \\ \hline 
			предсказание погоды & 10 \\ \hline 
			биомедицинские приложения & 9 \\ \hline 
			астрофизика и космология & 6  \\ \hline 
			молекулярная динамика   & 6   \\ \hline 
			мультифизика & 6              \\ \hline 
			геофизика & 6  \\ \hline 
			финансы & 2  \\ \hline 
		\end{tabular}
	\end{center}
	\label{tab_physics}
\end{table}
Комплексный тест производительности высокопроизводительных ВС должен включать в себя алгоритмы, аналогичные используемым в перечисленных приложениях по создаваемой нагрузке на ВС. Можно показать что метод частиц в ячейках соотвествует этому требованию и таким образом на основании проведенных расчетов по методу частиц в ячейках можно с высокой достоверностью предсказывать эффективность работы ВС на приложениях из приведенного списка. Ограничимся тремя наиболее актуальными (т.е. с наибольшим числом статей приложениями из приведенного списка) и пропуская физику плазмы:
\begin{itemize}
	\item \textbf{Вычислительная гидродинамика:} большая часть численных методов использует вычисления на сетке с регулярным доступом к памяти. Это означает, что нагрузка на процессорные элементы и оперативную память аналогична вычислению электромагнитных полей в методе частиц в ячейках
	\item \textbf{Ядерные технологии}: под этим название в обзоре фигурирует моделирование различных процессов, протекающих в ядерных реакторах выполняемое преимущественно на основе метода Монте-Карло, который является близким аналогом метода частиц в ячейках, с той разницей что для его работы требуется меньшее количество синхронизаций между параллельными процессами
	\item \textbf{Разработка новых материалов}: это квантовохимические расчеты структуры и поведения молекул, вычисление супер-многомерных интегралов в различных приближениях. Нагрузка создаваемая данными методами на ВС может достоверно эмулироваться соответвенно этапом расчета движения частиц и этапом вычисления электромагнитного поля в методе частиц в ячейках.
\end{itemize}	    	
Обобщая, можно сказать, что поскольку метод частиц в ячейках содержит участки кода с высокой вычислительной интенсивностью и с низкой, вычислительные процедуры с регулярным доступом к памяти и с нерегулярным, при использовании кэша разных уровней и без такового, а также параллельные коммуникации всех видов, то с использованием результатов тестирования ВС на методе частиц в ячейках можно предсказать эффективность реализации на данной ВС любого из существующих параллельных вычислительных методов и использующего этот метод физического приложения.

В \underline{\textbf{третьей главе}} описана методика измерения характеристик ВС с помощью программы, релизующей метод частиц в ячейках. 

Предложена методика комплексной оценки тестируемой ВС с точки зрения возможности эффективной реализзации математических моделей на основе определения баланса между скоростью счета и скоростью пересылки данных между узлами ВС. Баланс определяется на основе усреднения данных расчетов по методу частиц в ячейках, который используется в качестве оценки снизу по скорости счета и оценки сверху по памяти для большинства существующих математических методов.

Кроме того, на основе проведенных расчетов измерена скорость счета и скорость перемещения данных для нескольких протестированных ВС.

\textbf{О влиянии организации данных на результат измерения производительности процессоров}.
Модельные частицы расположены внутри расчетной области случайным образом. Если даже модельные частицы расположены рядом в массиве, где хранятся их координаты,  то сами значения координат будут близкими только вначале. В дальнейшем модельные частицы перемешиваются. Это означает, что обращения к трехмерным массивам, содержащим электрическое и магнитное поля, являются неупорядоченными,  и использование кэш-памяти в данном случае не позволяет сократить время счета. 

Были проведены вычислительные эксперименты с различными вариантами огранизации данных как о модельных частицах, так и об электромагнитных полях.

Основная ценность этих экспериментов в том, что установлено влияние, которое способ организации частиц в памяти оказывает на производительность, а значит и на результаты работы создаеваемой программы-теста, например, какое влияние на результат тестирования может оказать более или менее эффективно работающий кэш: по результатам, производительность может измениться н более чем в 1.5-2 раза.

{Расчет производительности системы памяти.}
Проведено измерение производительности системы памяти, основанное на измерении времени расчета движения модельных частиц, при этом благодаря алгоритмически особенностям метода частиц в ячейках удается исключить использование кэш-памяти и производить измерение скорости доступа именно к оперативной памяти.

Следует отметить, что вопрос о сравнении чисел на рис. \ref{PIC_RAM} с заявленной максимальной пропускной способностью 
является второстепенным, тем не менее, сравнение показано в таблице \ref{PIC_vs_PROC_RAM}. Основной вопрос в данном случае - это измерение пропускной способности памяти,  фактически доступной для расчетного приложения.

\begin{table}[ht]
	\caption{Сравнение пропускной способности памяти, измеренной с помощью теста на основе метода частиц с характеристиками процессора.}
	\label{PIC_vs_PROC_RAM}
	\begin{tabular}{|c|c|c|c|}
		\hline
		&            & \multicolumn{2}{|c|}{Пропускная способность памяти, GB/sec} \\ \cline{3-4}  	
		Название ВС  & Процессор  & Данные теста MANAS & Максимум \\ \hline
		МВС-100К     & Xeon E5450 &     6.02           & 21       \\ \hline 
		СКИФ-МГУ     & Xeon E5472 &     12.47          & 21       \\ \hline     
		СКИФ-Cyberia & Xeon 5150  &     1.45           & 10.6     \\ \hline
		Кластер НГУ  & Xeon 5355  &     4.47           & 21       \\ \hline
	\end{tabular}	
\end{table}
   

\textbf{Расчет производительности процессорных элементов.}
Для того, чтобы отделить время счета от времени обращения к оперативной памяти было рассмотрено время работы процедуры,
реализующей одномерное преобразование Фурье, которая является частью физической диагностики, используемой в при моделировании динамики плазмы. Измереннное время с учетом известного размера данных и и количества операций в БПФ \cite{FFT_OVS}, переводится во флопсы. Сравнительная производительность процессорных элементов некоторых из рассмотренных в диссертационной работе ВС выглядит как показано на рисунке  \ref{procs_flops}:

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/processor_FLOPS.png}
	\end{center}
	\caption{Производительность процессоров Intel Xeon, измеренная в ходе выполнения одномерного преобразования Фурье на некоторых кластерах. Размерность преобразования $N=64$. Измерения выполнены в 2010 г.}
	\label{procs_flops}
\end{figure}

\textbf{Расчет производительности процессорных элементов на основе движения модельных частиц.}
Сравнительная производительность процессорных элементов некоторых из рассмотренных в диссертационной работе ВС выглядит как показано на рисунке  \ref{procs_flops_pic}:

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=10cm,keepaspectratio]{images/processor_FLOPS_PIC.png}
	\end{center}
	\caption{Производительность процессоров Intel Xeon, измеренная на основе времени вычисления движения модельных частиц. Измерения выполнены в 2010 г.}
	\label{procs_flops_pic}
\end{figure} 

Расчет количества операций с плавающей точкой в секунду (FLOPS) при расчете движения модельных частиц $N_{PIC,FLOPS}$ производился следующим образом:
\begin{equation}
N_{PIC,FLOPS} = \frac{F_P\times N_P \times P_{core}}{\Delta t}
\label{PIC_FLOPS}
\end{equation}

здесь:
\begin{itemize}
	\item $F_P$ - количество операций на одну модельную частицу, $F_P = 500$;
	\item $N_P$ - количество модельных частиц на одно процессорное ядро (в рассмотренном случае $2.5\times 10^6$);  
	\item $P_{core}$ - количество ядер процессора;
	\item $\Delta t$  - длительность временного шага, сек.
\end{itemize}	

\begin{table}[ht]
	\caption{Производительность процессоров Intel Xeon, измеренная на основе времени вычисления движения модельных частиц.}
	\label{PIC_vs_PROC_RAM}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		&            &            &             &       \multicolumn{2}{|c|}{Производительность, GFLOPS} \\ \cline{5-6}  	
		Название ВС  & Процессор  &  $\Delta t$ &$P_{core}$ & Данные теста  &  \\
		&            &             &           & PIC-MANAS     & LU-разложение \\ \hline
		МВС-100К     & Xeon E5450 &  0.878      & 4     & 5.69           & 4.6     \\ \hline 
		СКИФ-МГУ     & Xeon E5472 &  0.423      & 4     & 11.83          & 2.37       \\ \hline     
		СКИФ-Cyberia & Xeon 5150  &  1.882      & 2     &  1.33          & 4.65    \\ \hline
		Кластер НГУ  & Xeon 5355  &  1.196      & 4     & 4.18           & 1.82       \\ \hline
	\end{tabular}	
\end{table}

 

\clearpage

\textbf{Расчет производительности коммуникационной сети.}
Разработана методика измерения быстродействия коммуникационной сети на основе анализа времени работы MPI-процедур, осуществляющих обмен граничными значениями между отдельными подобластями при решении уравнений Максвелла и при пересылке модельных частиц. В силу того, что при этом используются различные виды коммуникационных функций  - как блокирующие, так и не блокирующие, как парные, так и коллективные, при использовании эйлерово-лагранжевой декомпозиции - это позволяет набрать в течение одного расчета большую базу данных для получения знаний о структуре коммуникационной сети, времени прохождения сообщений в зависимоссти от размера, системных таймаутах и пр. 

\textbf{Оценка возможности выполнения крупномасштабных трехмерных расчетов}
Проведены тестовые расчеты с целью эффективности распараллеливания и масштабируемости разработанных алгоритмов для моделирования взаимодействия электронного пучка с плазмой, а также с целью выяснения реальных возможностей суперЭВМ по проведению физических расчетов. Показано, что текущая версия программы позволяет проводить расчет на трехмерной сетке размером 350$^3$ узлов при 100 частицах в ячейке за 1 сутки на 10 ядрах суперкомпьютера «Ломоносов», или 42 млн.узлов. Эффективность распараллеливания составила 92 \% для максимального использованного числа ядер: 100 для кластера НГУ и 200 для кластера «Политехник».  

Целью расчетов,  является ответ на вопрос, какие расчеты (т.е. какой размерности) могут бытть проведены на доступных высокопроизводительных  ВС (ВВС), на каком количестве процессоров (или процессорных ядер) и за какое время. 
Для того, чтобы получить ответ на этот вопрос, было запущено большое количество тестовых расчетов. Эти тестовые расчеты носят предварительный характер, и проводятся также с целью выяснения реальных возможностей ВВС для проведения физически содержательных расчетов. 	Расчеты проводились на следующих ВВС: кластер СпбПУ “Политехник”, кластер НКС-1П (ССКЦ СО РАН), кластер НГУ,  кластер НИВЦ МГУ “Ломоносов”. 

\textbf{Формула для комплексной оценки ВС.}
приведено обоснование формулы, на основании которой выносится оценка ВС по материалам проведенных тестов. При этом важно отметить, что оценка является не сравнительной - относительно других ВС, а абсолютной - с точки зрения математического моделирования. 

В частности, для того, чтобы параллельная ВС могла быть признана адаптированной к задачам математического моделирования, она должна соответвовать следующим требованиям:
\begin{enumerate}
	\item Относительно высокая производительность коммуникационной сети, позволяющая пересылать все необходимые для расчета данные, не задерживая вычислений
	\item Очень высокая пропускная способность дисковой подсистемы, обеспечивающая сохранение больших объемов данных, полученных в результате счета  	
\end{enumerate}

Важно отметить, что названы относительные показатели, обеспечивающие возможность пересылать и сохранять данные, без ущерба для скорости вычислений. Именно это и означает  комплексную пригодность ВС к решению задач математического моделирования, т.е. результаты счета сохраняются на диск с той же скоростью, с которой пересылаются данные между узлами данной ВС, и более того, эта скорость не намного меньше скорости вычислений.

Для того, чтобы все три упомянутые величины могли быть использованы в одной формуле, необходимо 
\begin{itemize}
	\item привести эти величины к одной размерности (скорость вычислений выражается во флопсах, скорость обмена данными - в гигабайтах в секунду)
	\item представить обобщенные коэффициенты, позволяющие сравнивать объем данных, сохраняемых на диск и объем данных, пересылаемых по коммуникационной сети ВС  
\end{itemize}

Для решения обоих этих задач использованы усредненные данные расчетов по методу частиц в ячейках на различных ВС. Данный метод может быть использован как оценка снизу, т.е. пригодность некоторой ВС для проведения расчетов по методу частиц в ячейках может трактоваться как возможность проведения расчетов по широкому спектру вычислительных методов, причем, как правило, с большей эффективностью.

Итак, коэффициент перевода из флопсов в байты в секунду для расчетов с частицами равен
$k_{f2b} = 500/576$ = 0.86   
и коээфициент для перевода объема данных, сохраняемых на диск к объему данных, пересылаемых по коммуникационной сети, аналогично, для частиц равен (усредненно):
$k_{MPI} = 0.05$ 
Это объясняется тем, что в среднем не более 5\% частиц пересылается между подобластями.
В итоге формула оценки $\xi$ имеет вид:
$$
\xi = \frac{W_{MPI}} {k_{f2b} W_{PIC}}, 
$$
при условии, что $W_{disc} \approx W_{MPI}$,
здесь $W_{disc}$ - скорость работы дисковой подсистемы (байт/сек), $W_{MPI}$
- скорость пересылки данных по сети - (байт/сек) и $W_{PIC}$ - скорость расчета по частицам (во флопсах).	




\underline{\textbf{Четвертая глава}} посвящена  
анализу масштабируемости, параллельной эффективности и ускорения параллельной ВС

Во четвертой главе предложена методика интегральной оценки тестируемой ВС с помощью измерения масштабируемости с расчетах по методу частиц в ячейках и определения на основе измерений возрастания потока данных в коммуникационной сети ВС.
%				На основе данных о пересылке модельных частиц измерена производительность коммуникационной сети ВС, при этом важно отметить преимущества использованного метода измерений: пересылка данных имеет высокую степень нерегулярности, а также большой объем, что означает проведение тестирования на большой нагрузке, и возможность широкого применения полученных таким образом данных.
%				Также предложена и апробирована методика определения фактически соседних (с точки зрения MPI) узлов ВС.  

%\subsubsection{Определение понятий эффективности, масштабируемости и ускорения}
%\textbf{Степаненко, книжка"высокопроизводительные вычисления", учебник Воеводина}

\textbf{Определение понятий эффективности, масштабируемости и ускорения.}
Выписано определение эффективности параллельной реализации программ, сильной и слабой масштабируемости, ускорения при распараллеливании.

В частности, ускорение параллельного алгоритма для $N$ процессоров, определяется как:
$$
S_N = \frac{T_1(n)}{T_N(n)}
$$  
здесь $T_1$ - время исполнения алгоритма на одном процессоре, $T_N$ - на $N$ процессорах, величина $n$ характеризует вычислительную сложность решаемой задачи, для определения ускорения существенно, что задачи, решаемые на 1 и на $N$ процессорах имеют одинаковую сложность.
Далее, эффективность распараллеливания вычисляется как
$$
\eta_N = \frac{1}{N}\frac{T_1(n)}{T_N(n)}
$$  
эта величина иногда называется эффективностью в сильном смысле (англ. strong efficiency). Кроме того, часто используется эффективность в слабом смысле (англ. weak efficiency):
\begin{equation}
\label{weak_eff}
\eta^{weak}_N = \frac{T_1(1)}{T_N(N)}
\end{equation}
здесь, в отличие от определения ускорения, задачи, решаемые на одном и на $N$ процессорах, имеют различную сложность, т.е. при увеличении количества процессоров в $N$ раз сложность задачи такжэе увеличивается в $N$ раз, и таким образом определяемая эффективность будет 100 \% в том случае, если время вычислений не возрастает при увеличении сложности задачи пропорциональном увеличению количества процессоров.   

Приведены различные используемые варианты этих определений, перечислены факторы, влияющие на  значения этих величин для конкретной ВС, а именно скорость обмена данными и структура коммуникационной сети ВВС, алгоритмы реализации MPI-процедур, в особенности коллективных, настройки коммуникационной системы (таймауты, размер системных буферов, и др.).
%\textbf{написать формулы} 
%\textbf{привести формулы из статьи в СибЖВМ}
Для последующего анализа измереннного времени работы метода частиц в ячейках на параллельной ВС, и для прояснения зависимости этого времени от параметров расчета можно привести следующую схематическую формулу для длительности одного временного шага, аналогично статье (Вшивков В.А. и др., СибЖВМ, 2003):

\begin{equation}
\label{PIC-timestep}
T=T_{F} \left ( \frac{N_x N_y N_z }{P_E}\right )+ T_{F,S}\left (N_y N_z\right ) + T_P\left(\frac{N_p}{P_E\times P_L}\right) +T_{P,S}\left (\frac{N_p*T_p}{P_E\times P_L}\right)
\end{equation}

здесь $T_{F}$ - время вычисления электромагнитного поля, $N_x, N_y, N_z$ - размер расчетной сетки соответственно, по координатам $X$ $Y$ и $Z$, $P_E$ - количество процессоров для эйлеровой декомпозиции, т.е. для разделения расчетной области на подобласти, $T_{F,S}$ - время, затрачиваемое на персылку граничных значений полей и токов между подобластями, которое зависит только от $N_y$ и $N_z$ в силу того что на данный момент используется одномерная декомпозиция,  $T_P$ - время расчета движения частиц, $P_L$ - количество процессов (или потоков) для лагранжевой декомпозиции, т.е. для дополнительного разделения частиц подобласти на группы, вычисляемые на отдельных ядрах.

{Анализ масштабируемости как интегральной характеристики ВС.}
Во втором разделе приведены фактически измеренные на различных высокопроизводительных ВС графики масштабируемости и параллельной эффективности и на основе этих данных проведен анлиз коммуникационной сети данных ВС, в частности, для МВС-100К, рис. \ref{eff2}. 

\begin{figure}[h]
	\begin{center}
		\includegraphics[height=5cm,keepaspectratio]{images/eff_weak_JSCC.png}
		\caption{
			Эффективность распараллеливания в слабом смысле, для МВС-100К, МСЦ РАН.
		}
		\label{eff2}
	\end{center} 
\end{figure}

Возможность проведения анализа коммуникационной сети с помщью расчетов по методу частиц в ячейках основана на известной информации о количестве пересылаемых данных и о виртуальной топологии, используемой в программе.

Размер данных, перемещаемых между двумя соседними MPI-процессами равен $144 \times N_y N_z  $ байт. При этом в идеальном случае, когда соседние MPI-процессы находятся на соседних узлах, коммуникации происходят только между соседними узлами, и поток данных в системе в целом не возрастает с ростом количества используемых в расчете узлов.

В частности, в расчете показанном на рис. \ref{eff2} использована эйлерова декомпозиция. Это означает, что используются только парные пересылки MPI, коллективные пересылки не используются, и поток данных через коммуникационную систему ВС в целом возрастать не должен. Если, тем не менее, он возрастает, что видно на рис. \ref{eff2} в виде снижения эффективности распараллеливания, то это может (при отсутствии коллективных операций), означать, что соседние с точки зрения MPI процессы находятся на физически удаленных друг от друга узлах параллельной ВС.

Обозначая $k_{||}$ зависимость коэффициента при времени пересылок граничных условий от количества процессоров в формуле \ref{PIC-timestep}, так что 
\begin{equation}
T_{F,S} = k_{||} (P_E) \frac{N_y N_z}{P_E}
\end{equation} 
и подставляя формулу \ref{PIC-timestep} d \ref{weak_eff}, рассматривая только лишь время расчета и пересылок электромагнитного поля, можно получить 
\begin{equation}
k_{||} (P_E) = \frac{1}{\eta^{weak}(P_E)} - 1
\end{equation}	  
Таким образом величина $k_{||} (P_E)$  - \textbf{степень нелинейности} коммуникационной структуры параллельной ВС. Фактически она представляет собой отклонение от линейной функции для зависимости времени пересылок от количества процессоров. Он показывает предел возрастания потока данных через коммуникационную структуру ВС при увеличении количества процессоров, используемых в расчете. Эта величина характеризует, в какой степени при передаче информации между соседними процессами в MPI используются узлы параллельной ВС, не являющиеся ближайшими соседями. В силу того, что на значение эффективности оказывает влияние не только свойства оборудования, но и особенност реализации MPI, возникает необходимость разделить эти факторы. Это достигается с помощью привязки процессорв к узлам. 

В итоге, такимобразом определенная  величина $k_{||} $ может быть использована как характеристика параллельной ВС, показывающая реально достижимую с помощью данной ВС эффективность и масштабируемость   

\textbf{Измерение продолжительности параллельных коммуникаций и анализ характеристик и топологии коммуникационного оборудования.}
В этом разделе описано решение задачи об определении соседства процессов по реальным узлам. Это исключительно важно для производительности реальных задач, чтобы виртуально близкие (т.е. по номеру MPI-процесса) процессы исполнялись бы на соседних узлах многопроцессорной ВС. Для этого проводится обмен сообщениями между узлами, выделенными 
для исполнения программы по топологии полного графа, и проводится анализ времени прохождения сообщений, рис. \ref{poly_all2all}.  

Следует отметить, что такого этапа, с обменом сообщениями между всеми процессами в рамках метода частиц нет, поэтому, такой анализ проводится предварительно, перед запуском основной части программы.


\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/polytech_all_to_all.png}
	\end{center}
	\caption{Время пересылок для 40 MPI-процессов, расположенных на 10 узлах по 4 процесса, кластер «Политехник», СПбПУ. По осям X и Y отложены номера процессов, цветовая шкала показывает время пересылок в секундах.}
	\label{poly_all2all}
\end{figure} 
Узлы с минимальным временем считаются близкими, т.е. выясняется фактическая топология ВС. Это сопоставляется с известной информацией о размещении процессов по узлам.	Для повышения производительности приложения в дальнейшем целесообразно передвинуть соседние процессы на те узлы, где по факту меньше задержка по коммуникациям.

%	\textbf{картинку из стьатьи все-со-всеми, (с Политеха)}. 
%	1.измерение всех видов MPI-коммуникаций, сравнение одного с другим (Send, Isend, Bsend) - и увязатиь это с алгоритмом
%	2. варьирование размера сообщений и пр. параметров

%материал статьи НГУ ИТ  с более аакуратным анализом

\textbf{Измерение производительности коммуникационной сети на основе данных о пересылке модельных частиц.}
В \textit{четвертом разделе} приведены результаты измерения скорости пересылок модельных частиц в методе частиц в ячейках и проведенный на основе этого сравнительный анализ скорости работы коммуникационной сети ВС.

На нескольких параллельных ВС были измерены времена, затраченные на пересылку модельных частиц между соседними узлами. Из статитики расчетов по методу частиц в ячейках известно, что пересылается, как правило, не более 5\% модельных частиц. В рассматриваемых расчетах размер сетки $512\times 64 \times 64$ узла  при 150 модельных частицах в ячейке, т.е.  на каждом временном шаге в среднем пересылается 15.7 млн. модельных частиц, что при размере одной частицы в 48 байт означает 720 Мб на каждый временной шаг, что при известном времени пересылки позволяет вычислить скорость. Таким образом была экспериментально измерена величина $T_{P,S}$, входящая в формулу \ref{PIC-timestep}   

\begin{figure}[htb]
	\begin{center}
		\includegraphics[height=7cm,keepaspectratio]{images/particle_send_GBsec.png}
	\end{center}
	\caption{Скорость пересылки данных на некоторых кластерах. Количество модельных частиц: 2.5 млн. на каждое процессорное ядро. Измерения выполнены в 2010 г.}
	\label{procs_flops}
\end{figure}

Измеренная таким образом скорость пресылки данных показывает фактический предел этой величины, реально достижимый для вычислительного приложения с использованием имеющегося оборудования и коммуникационного программного обеспечения. Это подтверждается следующими соображениями:
\begin{itemize}
	\item Объем пересылаемых данных мал: в среднем 6 Мб на процесс
	\item Соседние MPI-процессы, как правило, расположены на близких узлах.
\end{itemize}  	 


\underline{\textbf{Пятая глава}} посвящена 
анализу производительности узлов мультиархитектурной ВС.
В этой главе описаны методы, позволяющие определять скорость счета на ускорителях вычислений и скорость перемещения данных между ускорителем вычислений и хост-машиной, а также давать прогнозы о скорости счета нереализованных еще алгоритмов на тестируемой ВС.

Кроме того, предожена методика оценки качества узлов мультиархитектурной ВС на основе графических ( или других) ускорителей, при этом качество понимается как сбалнсированность средней оценочной скорости счета на ускорителе и скорости пермещения данныхмежду ускоритлем и хостом. 

\textbf{Анализ производительности узлов с графическмим ускорителями.}
редставлены результаты анализа производительности узлов с графическими ускорителями на основе измерения времени расчета движения модельных частиц, часть из котрых показана в таблице \ref{PerfGPU}.

\begin{table}[ht]
	\begin{center}
		\caption{Характеристики выполнения основных частей реализации метода частиц на современных GPU}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Название GPU                &  Tesla K80   & Tesla P100 \\ \hline
			Расчет электрического поля  &  25.3 мкс    &  16.059    \\ \hline
			Сдвиг частиц                &  348 мкс     &  338.16    \\ \hline
			Скорость копирования        &              &            \\
			частиц с хоста на GPU       & 4.92 ГБ/сек. &8.166 ГБ/сек.  \\ \hline
		\end{tabular}
		\label{PerfGPU}
	\end{center}
\end{table}

Основной вопрос данного раздела, как и всей работы - что можно узнать о данной ВС путем запуска программы, реализующей метод частиц в ячейках? В отличие от 	большинства других разделов информация о характеристиках оборудования в данном случае доступна через стандартный интерфейс, соответственно фактически измеренную скорость счета и скорость пересылки данных между хостом и GPU можно сравнивать с номинальными показателями.

Аналогично разделу \ref{calc_PE} определяется производительность GPU во флопсах как для этапу расчета частиц, так и для этапа расчета электромагнитного поля

Важнейшей интегральной характеристикой ВС, оснащенной графическими ускорителями, является возможность их полноценно использовать. Эта возможность
может быть измерена с помощью сопоставления вычисленной скорости счета и и скорости пересылок данных между хостом и GPU с использованием описанного 
в разделе \ref{complex_estimate} переводного множителя $k_{f2b}$. Этот множитель отражает принципиальную возможность переслать необходимые данные с GPU на хост и далее по коммуникационной сети ВС на соседние узлы раньше, чем они понадобятся для счета на соседнем узле, и таким образом счет может продолжать без задержек, вызванных комммуникациями.

Вместе с тем вопрос, который наиболее часто задают специалисты по математическому моделированию применительно к мультиархитектурной ВС, оснащенной графическими ускорителями - это возможность \textit{эффективной} реализации конкретного вычислительного алгоритма на даннной мультиархитектурной ВС.
Для ответа на данный вопрос предлагается интерполяционная формула:
\begin{equation}
v_{pre} = v_{PIC} k + (1-k) v_{B,E}
\end{equation} 
здесь $ v_{pre}$ - оценка скорости вычислений на GPU для рассматриваемого алгоритма, $v_{PIC}$ - скорость вычислений с на этапе сдвига модельных частиц, $v_{B,E}$ - на этапе расчета электромагнитного поля, а $k$ - интерполяционный множитель, получаемый из следующих соображений.

Как уже говорилось выше, большинство численных методов используемых в математическом моделировании находятся в промежуточном положении по отношению к используемым в методе частиц в ячейках алгоритму вычисления поля и алгоритму расчета движения частиц по следующим показателям:
\begin{itemize}
	\item вычислительной интенсивности (равномерное распределение вычислительно сложных фрагментов по тексту или отдельные высоконагруженные участки);
	\item характеру доступа к оперативнной памяти (регулярный или нерегулярный);
	\item объему используемых данных (большой или маленький).
\end{itemize}
Ориентировочное распределение вычислительных алгоритмов по рассмотренным показателям и соответствующие значения коэффициента $k$ показаны в таблице 


\begin{table}[ht]
	\begin{center}
		\caption{Определение интерполяционного коэффициента для некоторых типов вычислительных алгоритмов}
		\begin{tabular}{|c|c|c|c|c|}
			%	        &   &  &  & k \\ \hline
			\hline
			Вычислительный    & Интенсивность &  Доступ к           & Объем  & $k$  \\ 
			алгоритм          &               &  оперативной  & данных &  \\
			&               &  памяти       &        &  \\ \hline
			Расчет движения   &  низкая       & нерегулярный        & большой & 1.0 \\ 
			модельных частиц  &               &                     &          & \\\hline
			Метод Монте-Карло &  низкая       & нерегулярный        & средний & 0.9 \\ \hline
			Метод SPH         &  низкая       & нерегулярный        & небольшой & 0.6 \\ \hline	Метод             &  высокая      & нерегулярный        & большой & 0.5  \\
			конечных элементов &          &              &         & \\ \hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.2 \\ 		
			схемы (явные)      &           &            &         &     \\\hline
			Конечно-разностные &  высокая  & регулярный & большой & 0.1 \\ 		
			схемы (явные)-2    &           &            &         &     \\\hline
			Вычисление         &  высокая  & регулярный & большой & 0.0 \\ 		
			электромагнитного поля      &           &            &         &     \\\hline
			
			
		\end{tabular} 
		\label{tab-interp-koef}              
	\end{center}
\end{table}


%Отднльно рассматривается вопрос о скорости подкачки данных к мультипроцессоорам GPU, т.е. производительность шины памяти, а также скорость загрузки данных из памяти хоста в память GPU. \textbf{таблица из статьи Булл и рассуждения}, вывод о качестве GPU и особ- \textbf{о качестве соединения} 

\textbf{Анализ производительности узлов с многоядерными процессорами и ускорителями вычислений.} 
Представлены результаты анализа производительности узлов с многоядерными процессорами разных типов и ускорителями вычислений, построенными по технологии, совместимой с x86 (Intel Xeon Phi различных поколений)

Основные вопросы те же, что и разделе, посвященном графическим ускорителям: возможность полноценного использования вычислительной мощности 
ускорителей типа Intel Phi без задержек на перемещение данных и возможность эффективной реализации вычислительных алгоритмов на параллельной ВС, оснащенной ускорителми такого типа. Можно привести таблицу, аналогичную \ref{tab-interp-koef} с той поправкой, что эффективность многопоточного доступа к памяти на Intel Phi несколько ниже, поэтому значения интерполяционого коэффициента будут меньше для алгоритмов, использующих большой объем памяти.


В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

%При использовании пакета \verb!biblatex! список публикаций автора по теме
%диссертации формируется в разделе <<\publications>>\ файла
%\verb!../common/characteristic.tex!  при помощи команды \verb!\nocite!

\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
  \renewcommand{\bibname}{\large \authorbibtitle}
  \nocite{*}
  \insertbiblioauthor           % Подключаем Bib-базы
  %\insertbiblioother   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
  \ifnumgreater{\value{usefootcite}}{0}{
%  \nocite{*} % Невидимая цитата всех работ, позволит вывести все работы автора
  \insertbiblioauthorcited      % Вывод процитированных в автореферате работ автора
  }{
  \insertbiblioauthor           % Вывод всех работ автора
%  \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
%  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
  \insertbiblioother            % Вывод списка литературы, на которую ссылались в тексте автореферата
  }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
